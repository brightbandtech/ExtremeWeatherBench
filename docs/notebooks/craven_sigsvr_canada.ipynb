{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to show how we computed PPH and SigSVR in Canada with the LSR data there and to show how we selected our cases and the areas for them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install metpy version 1.6.3 for this notebook only\n",
    "!uv pip install metpy==1.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports you should need for this notebook\n",
    "import xarray as xr\n",
    "import metpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Add colorbar with same height as plot\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from typing import Optional, Union, Literal\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import requests\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# added by amy\n",
    "from cartopy.mpl.gridliner import LongitudeFormatter, LatitudeFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# make the basepath - change this to your local path\n",
    "basepath = Path.home() / 'ExtremeWeatherBench' / ''\n",
    "basepath = str(basepath) + '/'\n",
    "\n",
    "# ugly hack to load in our plotting scripts\n",
    "import sys\n",
    "sys.path.append(basepath + \"/docs/notebooks/\")\n",
    "import derived as derived\n",
    "from extremeweatherbench import inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read in and plot the data in Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is modified from the original code for CONUS to work with Canada LSR data\n",
    "def extract_lsr_data_canada(date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Pull the latest LSR data for a given date. We pull all reorts within 1 day. If date is none, we return all reports\n",
    "    \n",
    "    Args:\n",
    "        date: A pandas Timestamp object.\n",
    "    Returns:\n",
    "        df: A pandas DataFrame containing the LSR data with columns lat, lon, report_type, time, and scale.\n",
    "    \"\"\"\n",
    "\n",
    "    datafile = '/home/amy/CanadaLSRData_2020-2024.csv'\n",
    "\n",
    "    # Read the CSV files with all columns to identify report types\n",
    "    try:\n",
    "        df = pd.read_csv(datafile, delimiter=',', engine='python', parse_dates=['Date/Time UTC'], date_format='%Y-%m-%d %H:%M:%S')\n",
    "    except Exception as e:\n",
    "        print(f'Error pulling hail data for {date}: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # if date is none, presumably we want all reports\n",
    "    if (date == None):\n",
    "        return df\n",
    "\n",
    "    # Filter the DataFrame for the specified date range\n",
    "    spc_date = pd.Timestamp(date.year, date.month, date.day, 12, 00)  # Normalize to noon\n",
    "    start_date = spc_date    \n",
    "    end_date = spc_date + pd.Timedelta(days=1.0)\n",
    "    df = df[(df['Date/Time UTC'] >= start_date) & (df['Date/Time UTC'] < end_date)]\n",
    "    if len(df) == 0:\n",
    "        #print(f'No LSR data found for {start_date} to {end_date}')\n",
    "        return pd.DataFrame()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to make the main plotting function cleaner\n",
    "# helper function to convert a bounding box tuple to a shapely Polygon\n",
    "def get_polygon_from_bounding_box(bounding_box):\n",
    "    \"\"\"Convert a bounding box tuple to a shapely Polygon.\"\"\"\n",
    "    if bounding_box is None:\n",
    "        return None\n",
    "    left_lon, right_lon, bot_lat, top_lat = bounding_box\n",
    "    return Polygon(\n",
    "        [\n",
    "            (left_lon, bot_lat),\n",
    "            (right_lon, bot_lat),\n",
    "            (right_lon, top_lat),\n",
    "            (left_lon, top_lat),\n",
    "            (left_lon, bot_lat),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# helper function to plot a polygon on a cartopy axis\n",
    "def plot_polygon_outline(polygon, ax, color='yellow', alpha=0.5):\n",
    "    \"\"\"Plot a shapely Polygon on a Cartopy axis.\"\"\"\n",
    "    if polygon is None:\n",
    "        return\n",
    "    patch = patches.Polygon(\n",
    "        polygon.exterior.coords,\n",
    "        closed=True,\n",
    "        facecolor='none',\n",
    "        edgecolor=color,\n",
    "        alpha=alpha,\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "\n",
    "# imported from figure 1 notebook and adjusted to plot reports\n",
    "def plot_local_cases(left_lon, right_lon, bot_lat, top_lat, df, filename, plot_title, pph=None, sigsvr=None):\n",
    "    \"\"\"Plot the specified cases on a map of Canada.\n",
    "    Args:\n",
    "        left_lon: The left longitude of the bounding box.\n",
    "        right_lon: The right longitude of the bounding box.\n",
    "        bot_lat: The bottom latitude of the bounding box.\n",
    "        top_lat: The top latitude of the bounding box.\n",
    "        df: A pandas DataFrame containing the LSR data with columns 'Longitude', 'Latitude', and 'report_type'.\n",
    "        filename: The name of the file to save the plot.\n",
    "        plot_title: The title of the plot.\n",
    "        pph: A 2D array of probabilities for plotting (optional).\n",
    "        sigsvr: A 2D array of SIGSVR values for plotting (optional).\n",
    "    Returns:\n",
    "        None: The function saves the plot to the specified filename.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    #ax = plt.axes(projection=ccrs.LambertConformal())\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "    # Add coastlines and gridlines\n",
    "    ax.coastlines(zorder=10)\n",
    "    ax.add_feature(cfeature.BORDERS, edgecolor='black', zorder=9)\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='grey', zorder=5)\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='black', zorder=3)\n",
    "    ax.add_feature(cfeature.LAKES, edgecolor='black', facecolor='white', zorder=2)\n",
    "    # plot the states\n",
    "    #ax.add_feature(cfeature.OCEAN, zorder=10)\n",
    "    #utils.remove_ocean_gridpoints(sigsvr)\n",
    "    my_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "    ax.set_extent(my_box, crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Add gridlines\n",
    "    gl = ax.gridlines(draw_labels=True)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xformatter = LongitudeFormatter()\n",
    "    gl.yformatter = LatitudeFormatter()\n",
    "\n",
    "    if (pph is not None):\n",
    "        # Plot the data using contourf\n",
    "        levels = [0.01, .05,.15,.30,.45,.60,.75] \n",
    "\n",
    "        # Create a custom colormap that sets alpha=0 for values below 0.05\n",
    "        cmap = plt.cm.viridis\n",
    "        norm = mcolors.BoundaryNorm(levels, cmap.N)\n",
    "\n",
    "        # Create the colormap with alpha=0 for values below 0.05\n",
    "        # Create a mask for values below 0.05\n",
    "        mask = np.ma.masked_less(pph, 0.01)\n",
    "        cmap_with_alpha = plt.cm.viridis.copy()\n",
    "        cmap_with_alpha.set_bad('none', alpha=0)  # Set masked values to transparent\n",
    "\n",
    "        contour = ax.contour(pph.longitude, pph.latitude, mask, \n",
    "                            levels=levels, transform=ccrs.PlateCarree(),\n",
    "                            cmap=cmap_with_alpha, norm=norm, extend='both', zorder=15)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(contour, ax=ax, orientation='horizontal', pad=0.05, shrink=0.8, spacing='proportional')\n",
    "        cbar.lines[0].set_linewidth(10.0)\n",
    "        cbar.set_label('Probability')\n",
    "\n",
    "    if (sigsvr is not None):\n",
    "        masked_sigsvr = sigsvr.where(sigsvr >= 500)\n",
    "        print(masked_sigsvr.max())\n",
    "        im = masked_sigsvr.plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Reds',\n",
    "                        add_colorbar=False, vmin=0)\n",
    "        cbar = fig.colorbar(im, label=\"Craven SigSvr (m続/s続)\")\n",
    "        cbar.set_label(\"Craven SigSvr (m続/s続)\", size=14)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Make sure reports are visible by increasing size and using a distinctive color\n",
    "    # Convert string coordinates to float before plotting\n",
    "    colors = {'tor': 'red','wind': 'blue', 'hail': 'black'}\n",
    "    markers= {'tor': 'o', 'wind': 's', 'hail': '^'}  \n",
    "\n",
    "    event_colors = {'non_tornadic_vortex': 'pink', 'unclassified_wind_damage': 'blue', \n",
    "                    'tornado_over_land': 'red', 'downburst': 'darkblue', 'tornado_over_water': 'lightcoral', \n",
    "                    'unclassified_visual_vortex': 'orange', 'nan': 'gray'}\n",
    "\n",
    "    # Define zorder values to control plotting order (higher values appear on top)\n",
    "    zorders = {'tor': 22, 'wind': 21, 'hail': 20}\n",
    "    \n",
    "    # Sort the dataframe by report type to ensure tornadoes are plotted last (on top)\n",
    "    # Create a custom sort order where 'tor' comes last\n",
    "    sort_order = {'hail': 0, 'wind': 1, 'tor': 2}\n",
    "    sorted_df = df.copy()\n",
    "    sorted_df['sort_key'] = sorted_df['report_type'].map(sort_order)\n",
    "    \n",
    "    # Group by report type and plot each group with its own color\n",
    "    for report_type, group in sorted_df.sort_values('sort_key').groupby('report_type'):\n",
    "        ax.scatter(group['Longitude'].astype(float), group['Latitude'].astype(float), \n",
    "                  color=colors.get(report_type, 'gray'), s=20, marker=markers.get(report_type), alpha=0.9,\n",
    "                  transform=ccrs.PlateCarree(), label=f'{report_type.capitalize()} Reports', \n",
    "                  zorder=zorders.get(report_type, 10))\n",
    "\n",
    "    # Count the number of reports for each type\n",
    "    counts = df['report_type'].value_counts().to_dict()\n",
    "    print(counts)\n",
    "\n",
    "    # Create custom legend handles\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker=markers.get(label), color=colors[label], label=f'{label} (n={count})', markerfacecolor=colors[label], markersize=10)\n",
    "        for label, count in counts.items()\n",
    "    ]\n",
    "    \n",
    "    \n",
    "     # Create a larger legend by specifying a larger font size in the prop dictionary\n",
    "    ax.legend(handles=legend_elements, loc='upper left', framealpha=1, frameon=True, borderpad=0.5, handletextpad=0.8, handlelength=2.5)\n",
    "    ax.set_title(\"\")\n",
    "    ax.set_title(plot_title, loc='left', fontsize=20)\n",
    "\n",
    "    if (filename is not None):\n",
    "        plt.savefig(filename, transparent=False, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test of all points in Canada\n",
    "# plot North America\n",
    "bot_lat = 40\n",
    "top_lat = 85\n",
    "left_lon = -152\n",
    "right_lon = -45\n",
    "\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "plot_title = 'LSRs in Canada'\n",
    "\n",
    "can_data = extract_lsr_data_canada(date=None)  # Get all reports\n",
    "#print(can_data)\n",
    "\n",
    "# set filename to the file if you want to actually save it\n",
    "plot_local_cases(left_lon, right_lon, bot_lat, top_lat, can_data, filename=basepath + 'docs/notebooks/figs/LSR_Canada.png', plot_title=plot_title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practically_perfect_hindcast_weighted(\n",
    "    df = None,\n",
    "    resolution: float = 0.25,\n",
    "    report_type: Union[Literal[\"all\"], list[Literal[\"tor\", \"hail\", \"wind\"]]] = \"all\",\n",
    "    sigma: float = 1.5,\n",
    "    return_reports: bool = False,\n",
    "    output_resolution: Optional[float] = None,\n",
    "    report_constants = {'hail': 1, 'tor': 1}\n",
    ") -> Union[xr.DataArray, tuple[xr.DataArray, pd.DataFrame]]:\n",
    "    \"\"\"Compute the Practically Perfect Hindcast (PPH) using storm report data using latitude/longitude grid spacing\n",
    "    instead of the NCEP 212 Eta Lambert Conformal projection; based on the method described in Hitchens et al 2013,\n",
    "    https://doi.org/10.1175/WAF-D-12-00113.1\n",
    "\n",
    "    Args:\n",
    "        date: A pandas Timestamp object.\n",
    "        resolution: The resolution of the grid to use. Default is 0.25 degrees.\n",
    "        report_type: The type of report to use. Default is all. Currently only supports all.\n",
    "        sigma: The sigma (standard deviation) of the gaussian filter to use. Default is 1.5.\n",
    "        return_reports: Whether to return the reports used to compute the PPH. Default is False.\n",
    "        output_resolution: The resolution of the output grid. Default is None (keep the same resolution as the input grid).\n",
    "        report_constants: A dictionary mapping report types to their respective values for the PPH grid. Default is {'hail': 15, 'tor': 5}.\n",
    "    Returns:\n",
    "        pph: An xarray DataArray containing the PPH around the storm report data.\n",
    "    \"\"\"\n",
    "\n",
    "    #df = extract_lsr_data_canada(date)\n",
    "    if report_type == \"all\":\n",
    "        pass\n",
    "    else:\n",
    "        df = df[df['report_type'].isin(report_type)]\n",
    "\n",
    "    # Create a grid covering Canada\n",
    "    lat_min, lat_max = 10, 85   \n",
    "    lon_min, lon_max = -152, -45  \n",
    "\n",
    "    # Create the grid coordinates\n",
    "    grid_lats = np.arange(lat_min, lat_max + resolution, resolution)\n",
    "    grid_lons = np.arange(lon_min, lon_max + resolution, resolution)\n",
    "\n",
    "    # Initialize an empty grid\n",
    "    grid = np.zeros((len(grid_lats), len(grid_lons)))\n",
    "\n",
    "    # extract reports for TOR and HAIL separately to handle the underreporting\n",
    "    for report_type in report_constants.keys():\n",
    "        # Filter the dataframe for the current report type\n",
    "        df2 = df[df['report_type'] == report_type]\n",
    "\n",
    "        # Extract latitude and longitude from the dataframe\n",
    "        lats = df2[\"Latitude\"].astype(float)\n",
    "        lons = df2[\"Longitude\"].astype(float)\n",
    "\n",
    "        # Mark grid cells that contain reports\n",
    "        for lat, lon in zip(lats, lons):\n",
    "            # Find the nearest grid indices\n",
    "            lat_idx = np.abs(grid_lats - lat).argmin()\n",
    "            lon_idx = np.abs(grid_lons - lon).argmin()\n",
    "            grid[lat_idx, lon_idx] = report_constants[report_type]  # Set a value to indicate a report is present\n",
    "\n",
    "    # Create the xarray DataArray\n",
    "    pph = xr.DataArray(\n",
    "        grid,\n",
    "        dims=[\"latitude\", \"longitude\"],\n",
    "        coords={\"latitude\": grid_lats, \"longitude\": grid_lons},\n",
    "        name=\"practically_perfect\",\n",
    "    )\n",
    "\n",
    "    # Apply bilinear interpolation to smooth the field\n",
    "    # First, create a gaussian kernel for smoothing\n",
    "    smoothed_grid = gaussian_filter(grid, sigma=sigma)\n",
    "\n",
    "    # Replace the data in the DataArray\n",
    "    pph.data = smoothed_grid\n",
    "    if output_resolution is not None:\n",
    "        pph = pph.interp(latitude=np.arange(lat_min, lat_max + output_resolution, output_resolution),\n",
    "                        longitude=np.arange(lon_min, lon_max + output_resolution, output_resolution),method='linear')\n",
    "    if return_reports:\n",
    "        return (pph, df)\n",
    "    return pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a specific test case with PPH to verify that it is working well with the chosen weights\n",
    "date = pd.Timestamp(2021,7,15,12,00)\n",
    "daily_data = extract_lsr_data_canada(date=date)  # Get all reports\n",
    "\n",
    "pph = practically_perfect_hindcast_weighted(daily_data, sigma=5, report_constants={'tor': 10, 'hail': 10})\n",
    "\n",
    "#print(daily_data)\n",
    "    \n",
    "# plot PPH and the cases\n",
    "plot_local_cases(left_lon, right_lon, bot_lat, top_lat, daily_data, None, plot_title, pph=pph)\n",
    "\n",
    "#print(pph)\n",
    "#pph.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all dates from 2020-2024\n",
    "for date in pd.date_range('2020-01-01', '2024-12-31'):\n",
    "     # Filter the DataFrame for the specified date range\n",
    "    daily_data = extract_lsr_data_canada(date=date)  # Get all reports    \n",
    "\n",
    "    if len(daily_data) >= 10:\n",
    "        # Count the number of reports for each type\n",
    "        counts = daily_data['report_type'].value_counts().to_dict()\n",
    "        # make sure there is a tor count greater than 0\n",
    "        # if there is, plot the PPH and the cases\n",
    "        if (counts.get('tor', 0) > 0):\n",
    "            print(f\"{date.strftime('%Y-%m-%d')}: {len(daily_data)} reports - {counts}\")\n",
    "\n",
    "            filename = basepath + f\"docs/notebooks/figs/Canada_LSR_{date.strftime('%Y-%m-%d')}.png\"\n",
    "            plot_title = f\"LSRs focused on Canada for {date.strftime('%Y-%m-%d')} 12Z to {(date + pd.Timedelta(days=1.0)).strftime('%Y-%m-%d %H:%M')} 12Z\"\n",
    "\n",
    "            # compute PPH on this date\n",
    "            pph = practically_perfect_hindcast_weighted(daily_data, sigma=5, report_constants={'tor': 10, 'hail': 10})\n",
    "\n",
    "            # plot PPH and the cases\n",
    "            plot_local_cases(left_lon, right_lon, bot_lat, top_lat, daily_data, filename, plot_title, pph=pph)\n",
    "            #plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the US data so we can compare across the border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_lsr_data(date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Pull the latest LSR data for a given date. A \"date\" for LSRs is considered the\n",
    "    date starting at 12 UTC to the next day at 11:59 UTC.\n",
    "    \n",
    "    Args:\n",
    "        date: A pandas Timestamp object.\n",
    "    Returns:\n",
    "        df: A pandas DataFrame containing the LSR data with columns lat, lon, report_type, time, and scale.\n",
    "    \"\"\"\n",
    "    # Try the filtered URL first, if it fails, try without _filtered\n",
    "    url = f\"https://www.spc.noaa.gov/climo/reports/{date.strftime('%y%m%d')}_rpts_filtered.csv\"\n",
    "    # Check if the URL exists by attempting to open it\n",
    "    response = requests.head(url)\n",
    "    if date < pd.Timestamp('2004-02-29'):\n",
    "        raise ValueError(\"LSR data before 2004-02-29 is not available in CSV format\")\n",
    "    if response.status_code == 404:\n",
    "        # If the filtered URL doesn't exist, use the non-filtered version\n",
    "        url = f\"https://www.spc.noaa.gov/climo/reports/{date.strftime('%y%m%d')}_rpts.csv\"\n",
    "    # Read the CSV file with all columns to identify report types\n",
    "    try:\n",
    "        df = pd.read_csv(url, delimiter=',', engine='python', names=['Time','Scale','Location','County','State','Lat','Lon','Comments'])\n",
    "    except Exception as e:\n",
    "        print(f'Error pulling LSR data for {date}: {e}')\n",
    "        return pd.DataFrame()\n",
    "    if len(df) == 3:\n",
    "        return pd.DataFrame()\n",
    "    # Initialize report_type column\n",
    "    df['report_type'] = None\n",
    "    \n",
    "    # Find rows with headers and mark subsequent rows with appropriate report type\n",
    "    for i, row in df.iterrows():\n",
    "        if 'F_Scale' in row.values:\n",
    "            df.loc[i+1:, 'report_type'] = 'tor'\n",
    "        elif 'Speed' in row.values:\n",
    "            df.loc[i+1:, 'report_type'] = 'wind'\n",
    "        elif 'Size' in row.values:\n",
    "            df.loc[i+1:, 'report_type'] = 'hail'\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    df = df[['Lat', 'Lon', 'report_type','Time', \"Scale\"]]\n",
    "    # Remove rows that have 'Lat' in the 'Lat' column (these are header rows)\n",
    "    df = df[df['Lat'] != 'Lat']\n",
    "    time = pd.to_datetime(df['Time'], format='%H%M').dt.time\n",
    "    df['Time'] = pd.to_datetime(date.strftime('%Y-%m-%d') + ' ' + time.astype(str))\n",
    "    df = df.rename(columns={'Lat': 'lat', 'Lon': 'lon','Time':'time'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extremeweatherbench import evaluate, utils, cases, defaults\n",
    "\n",
    "# load in all of the events in the yaml file\n",
    "case_dict = utils.load_events_yaml()\n",
    "\n",
    "# turn the dictionary into a list of case objects\n",
    "ewb_cases = cases.load_individual_cases(case_dict)\n",
    "\n",
    "# build out all of the expected data to evalate the case\n",
    "# this will not be a 1-1 mapping with ewb_cases because there are multiple data sources to evaluate for some cases\n",
    "# for example, a heat/cold case will have both a case operator for ERA-5 data and GHCN\n",
    "case_operators = cases.build_case_operators(case_dict, defaults.get_brightband_evaluation_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert a bounding box tuple to a shapely Polygon\n",
    "def get_polygon_from_bounding_box(bounding_box):\n",
    "    \"\"\"Convert a bounding box tuple to a shapely Polygon.\"\"\"\n",
    "    if bounding_box is None:\n",
    "        return None\n",
    "    left_lon, right_lon, bot_lat, top_lat = bounding_box\n",
    "    return Polygon(\n",
    "        [\n",
    "            (left_lon, bot_lat),\n",
    "            (right_lon, bot_lat),\n",
    "            (right_lon, top_lat),\n",
    "            (left_lon, top_lat),\n",
    "            (left_lon, bot_lat),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now loop through the cases in the US and plot the data from Canada for the US dates on a North America map\n",
    "# plot North America\n",
    "bot_lat = 20\n",
    "top_lat = 85\n",
    "left_lon = -152\n",
    "right_lon = -45\n",
    "\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "plot_title = 'LSRs in Canada and the US'\n",
    " # save the bounding box polygon to subset the counts later\n",
    "bounding_box_polygon = get_polygon_from_bounding_box(bounding_box)\n",
    "\n",
    "for indiv_case in ewb_cases.cases:\n",
    "    indiv_event_type = indiv_case.event_type\n",
    "    # only look at the severe events\n",
    "    if (indiv_event_type != 'severe_convection'):\n",
    "        continue\n",
    "\n",
    "    # check if the case is inside the bounding box\n",
    "    if bounding_box is not None:\n",
    "        if (not shapely.intersects(indiv_case.location.geopandas.geometry[0], bounding_box_polygon)):\n",
    "            print(f\"Skipping case {indiv_case.case_id_number} as it is outside the bounding box.\")\n",
    "            continue\n",
    "\n",
    "    date = indiv_case.start_date\n",
    "    print(f\"Processing case {indiv_case.case_id_number} on {date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    can_data = extract_lsr_data_canada(date)\n",
    "    #print(can_data)\n",
    "    us_data = pull_lsr_data(date)\n",
    "    us_data.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n",
    "    #print(us_data)\n",
    "    # drop all the wind reports to focus on hail and tor\n",
    "    us_data = us_data[us_data['report_type'] != 'wind']\n",
    "\n",
    "    joint_data = pd.concat([can_data, us_data], ignore_index=True)\n",
    "\n",
    "    # set filename to the file if you want to actually save it\n",
    "    filename = basepath + f\"docs/notebooks/figs/Canada_US_LSR_{date.strftime('%Y-%m-%d')}.png\"\n",
    "    plot_title = f\"LSRs in Canada and the US using US dates for {date.strftime('%Y-%m-%d')} 12Z to {(date + pd.Timedelta(days=1.0)).strftime('%Y-%m-%d %H:%M')} 12Z\"\n",
    "\n",
    "    pph = practically_perfect_hindcast_weighted(joint_data, sigma=5, report_constants={'tor': 10, 'hail':10})\n",
    "\n",
    "    plot_local_cases(left_lon, right_lon, bot_lat, top_lat, joint_data, filename=filename, plot_title=plot_title, pph=pph)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot North America\n",
    "bot_lat = 10\n",
    "top_lat = 85\n",
    "left_lon = -152\n",
    "right_lon = -45\n",
    "\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "\n",
    "# do the opposite and look for Canadian event days and then plot the US data with it\n",
    "# look through every date in the australia data print the case count for all non-zero dates\n",
    "can_data = extract_lsr_data_canada(date=None)  # Get all reports\n",
    "\n",
    "# loop through all dates from 2020-2024\n",
    "for date in pd.date_range('2020-01-01', '2024-12-31'):\n",
    "    # grab the canada data for this date\n",
    "    daily_data = extract_lsr_data_canada(date=date)  # Get all reports\n",
    "\n",
    "    if len(daily_data) >= 10:\n",
    "        # Count the number of reports for each type\n",
    "        counts = daily_data['report_type'].value_counts().to_dict()\n",
    "        # make sure there is a tor count greater than 0\n",
    "        # if there is, plot the PPH and the cases\n",
    "        if (counts.get('tor', 0) > 0):\n",
    "            print(f\"{date.strftime('%Y-%m-%d')}: {len(daily_data)} reports - {counts}\")\n",
    "\n",
    "            filename = basepath + f\"docs/notebooks/figs/Canada_LSR_{date.strftime('%Y-%m-%d')}.png\"\n",
    "            plot_title = f\"LSRs in Canada for {date.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "            us_data = pull_lsr_data(date)\n",
    "            us_data.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n",
    "            #print(us_data)\n",
    "            # drop all the wind reports to focus on hail and tor\n",
    "            if (us_data is not None) and (len(us_data) > 0):\n",
    "                us_data = us_data[us_data['report_type'] != 'wind']\n",
    "\n",
    "            joint_data = pd.concat([daily_data, us_data], ignore_index=True)\n",
    "\n",
    "            # set filename to the file if you want to actually save it\n",
    "            filename = basepath + f\"docs/notebooks/figs/Canada_US_LSR_{date.strftime('%Y-%m-%d')}.png\"\n",
    "            plot_title = f\"LSRs in Canada and US using Canada events for {date.strftime('%Y-%m-%d')} 12Z to {(date + pd.Timedelta(days=1.0)).strftime('%Y-%m-%d %H:%M')} 12Z\"\n",
    "\n",
    "            pph = practically_perfect_hindcast_weighted(joint_data, sigma=5, report_constants={'tor': 10, 'hail':10})\n",
    "\n",
    "            plot_local_cases(left_lon, right_lon, bot_lat, top_lat, joint_data, filename=filename, plot_title=plot_title, pph=pph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot North America\n",
    "bot_lat = 30\n",
    "top_lat = 55\n",
    "left_lon = -95\n",
    "right_lon = -70\n",
    "    \n",
    "# plot a specific test case with PPH to verify that it is working well with the chosen weights\n",
    "date = pd.Timestamp(2020,6,10,00,00)\n",
    "daily_data = extract_lsr_data_canada(date=date)  # Get all reports\n",
    "\n",
    "us_data = pull_lsr_data(date)\n",
    "us_data.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n",
    "#print(us_data)\n",
    "# drop all the wind reports to focus on hail and tor\n",
    "if (us_data is not None) and (len(us_data) > 0):\n",
    "    us_data = us_data[us_data['report_type'] != 'wind']\n",
    "\n",
    "joint_data = pd.concat([daily_data, us_data], ignore_index=True)\n",
    "\n",
    "# set filename to the file if you want to actually save it\n",
    "filename = basepath + f\"docs/notebooks/figs/Canada_US_LSR_{date.strftime('%Y-%m-%d')}.png\"\n",
    "#plot_title = f\"LSRs in Canada and US using Canada events for {date.strftime('%Y-%m-%d')} 12Z to {(date + pd.Timedelta(days=1.0)).strftime('%Y-%m-%d %H:%M')} 12Z\"\n",
    "plot_title = \"LSRs 2020-06-10 12Z to 2020-06-11 12Z\"\n",
    "\n",
    "pph = practically_perfect_hindcast_weighted(joint_data, sigma=5, report_constants={'tor': 10, 'hail':10})\n",
    "\n",
    "plot_local_cases(left_lon, right_lon, bot_lat, top_lat, joint_data, filename=None, plot_title=plot_title, pph=pph)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
