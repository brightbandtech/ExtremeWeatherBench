{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae34bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports you should need for this notebook\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only needs to be run ONCE to fix the broken datetimes in the hail and tornado data.\n",
    "# It will create new files with the fixed datetimes and a combined file for sharing.\n",
    "# If you run this again, it will overwrite the existing files, so be careful!\n",
    "\n",
    "# the combined file is uploaded to the google bucket in the ExtremeWeatherBench project\n",
    "# and can be accessed at gs://extremeweatherbench/datasets/AustralianLSRData_2020-2024.csv\n",
    "\n",
    "def parse_datetime(row):\n",
    "    \"\"\"\n",
    "    Parses date and time components from a DataFrame row into a single datetime object.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a row of the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A datetime object.\n",
    "    \"\"\"\n",
    "    # Assuming a fixed year for simplicity, adjust as needed.\n",
    "    padded_day = str(row['Day']).zfill(2)\n",
    "    padded_month = str(row['Month']).zfill(2)\n",
    "    padded_time = str(row['Time (UTC)']).zfill(4)\n",
    "    date_string = f\"{padded_month}/{padded_day}/{row['Year']} {padded_time}\"\n",
    "    #print(f\"Parsing date string: {date_string}\")  # Debugging line\n",
    "    return datetime.strptime(date_string, '%m/%d/%Y %H%M')\n",
    "\n",
    "\n",
    "# # fix the broken datetimes in the australian hail data\n",
    "# hail_file = \"/home/amy/NHP_edited.csv\"\n",
    "# df_hail = pd.read_csv(hail_file, delimiter=',', engine='python', parse_dates=['Date & Time of Observation'], infer_datetime_format=True)\n",
    "\n",
    "# # rename the date column to match the tornado data\n",
    "# df_hail.rename(columns={'x': 'Longitude', 'y': 'Latitude'}, inplace=True)\n",
    "# df_hail.rename(columns={'Date & Time of Observation': 'Date/Time UTC'}, inplace=True)\n",
    "# df_hail = df_hail[['Date/Time UTC', 'Latitude', 'Longitude', 'Event Name', \"Maximum Hail Dimension (mm)\"]]\n",
    "\n",
    "# # Crate a report_type column\n",
    "# df_hail['report_type'] = 'hail'\n",
    "\n",
    "# # save the new file\n",
    "# new_hail_file = '/home/amy/NHP_2020-2024_cleaned_fixed.csv'\n",
    "# df_hail.to_csv(new_hail_file, index=False)\n",
    "\n",
    "# read in the second hail report file\n",
    "hail_file2 = \"/home/amy/integrated_canadian_hail_db_v1.1_cleaned.csv\"\n",
    "df_hail2 = pd.read_csv(hail_file2, delimiter=',', engine='python', parse_dates=['Start Time'], infer_datetime_format=True)\n",
    "\n",
    "# rename the date column to match the tornado data\n",
    "df_hail2.rename(columns={'Start Time': 'Date/Time UTC', 'Hail Diameter (mm)': 'Maximum Hail Dimension (mm)'}, inplace=True)\n",
    "df_hail2 = df_hail2[['Date/Time UTC', 'Latitude', 'Longitude', 'Reference Object', 'Maximum Hail Dimension (mm)']]\n",
    "\n",
    "# Crate a report_type column\n",
    "df_hail2['report_type'] = 'hail'\n",
    "\n",
    "# save the new file\n",
    "new_hail_file2 = '/home/amy/integrated_canadian_hail_db_2020-2022_fixed.csv'\n",
    "df_hail2.to_csv(new_hail_file2, index=False)\n",
    "\n",
    "# create a new hail dataframe with only severe reports (>= 20 mm)\n",
    "df_hail_combined = pd.concat([df_hail2], ignore_index=True)\n",
    "df_hail_severe = df_hail_combined[df_hail_combined['Maximum Hail Dimension (mm)'] >= 20]\n",
    "\n",
    "# save the new file\n",
    "new_hail_severe_file = '/home/amy/NHP_2020-2024_severe_fixed.csv'\n",
    "df_hail_severe.to_csv(new_hail_severe_file, index=False)\n",
    "\n",
    "# fix the broken date-times in the tornado data\n",
    "tor_file = \"/home/amy/NTP_edited2.csv\"\n",
    "\n",
    "# parse the dates from the Day\tMonth\tTime (UTC) columns\n",
    "df_tor = pd.read_csv(tor_file, delimiter=',', engine='python')\n",
    "# Apply the function to create a new 'Datetime' column\n",
    "df_tor['Date/Time UTC'] = df_tor.apply(parse_datetime, axis=1)\n",
    "df_tor.rename(columns={'x': 'Longitude', 'y': 'Latitude'}, inplace=True)\n",
    "\n",
    "df_tor = df_tor[['Date/Time UTC', 'Latitude', 'Longitude', 'Event Name', 'Event Type', \"Damage\"]]\n",
    "\n",
    "# subselect only the torando events on the ground (no waterspouts etc)\n",
    "df_tor = df_tor[df_tor['Event Type'] == 'tornado_over_land']\n",
    "\n",
    "# Crate a report_type column\n",
    "df_tor['report_type'] = 'tor'\n",
    "\n",
    "# save the new file\n",
    "new_tor_file = '/home/amy/CanadaTor2020-2024_cleaned_fixed.csv'\n",
    "df_tor.to_csv(new_tor_file, index=False)\n",
    "\n",
    "# create a combined file for sharing\n",
    "df_combined = pd.concat([df_hail_severe, df_tor], ignore_index=True)\n",
    "combined_file = '/home/amy/CanadaLSRData_2020-2024.csv'\n",
    "\n",
    "# save the combined file\n",
    "df_combined.to_csv(combined_file, index=False)\n",
    "\n",
    "print (\"DONE\")\n",
    "#print the types of the dataframe\n",
    "print(df_combined.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
