{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0915e674",
   "metadata": {},
   "source": [
    "# EWB paper Figure 2: Observations \n",
    "\n",
    "We provide the exact code used to generate each figure in order to be completely reproducible and to encourage others to use EWB with their own models quickly. \n",
    "This is Figure 2, it shows all of the cases using an outline and then highlights the observations available for each inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e6f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d227d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup all the imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager\n",
    "flist = matplotlib.font_manager.get_font_names()\n",
    "from tempfile import NamedTemporaryFile\n",
    "from extremeweatherbench import evaluate, utils, cases, defaults\n",
    "import shapely\n",
    "from typing import Optional, Union, Literal\n",
    "import xarray as xr\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# make the basepath - change this to your local path\n",
    "basepath = Path.home() / 'ExtremeWeatherBench' / ''\n",
    "basepath = str(basepath) + '/'\n",
    "\n",
    "# ugly hack to load in our plotting scripts\n",
    "import sys\n",
    "sys.path.append(basepath + \"/docs/notebooks/\")\n",
    "import case_plotting as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d05c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function is deprecated and will be removed in a future release. Please use cases.load_ewb_events_yaml_into_case_collection instead.\n",
      "This function is deprecated and will be removed in a future release. Please use cases.read_incoming_yaml instead.\n"
     ]
    }
   ],
   "source": [
    "# load in all of the events in the yaml file\n",
    "case_dict = utils.load_events_yaml()\n",
    "\n",
    "# turn the dictionary into a list of case objects\n",
    "ewb_cases = cases.load_individual_cases(case_dict)\n",
    "\n",
    "# build out all of the expected data to evalate the case\n",
    "# this will not be a 1-1 mapping with ewb_cases because there are multiple data sources to evaluate for some cases\n",
    "# for example, a heat/cold case will have both a case operator for ERA-5 data and GHCN\n",
    "case_operators = cases.build_case_operators(case_dict, defaults.get_brightband_evaluation_objects())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc34c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"valid_time\"; valid columns: [\"surface_air_temperature\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'select' <---\nSELECT [col(\"surface_air_temperature\")]\n  Parquet SCAN [gs://extremeweatherbench/datasets/ghcnh_all_2020_2024.parq]\n  PROJECT */15 COLUMNS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/amy/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/amy/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/amy/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_22058/3258851479.py\", line 8, in <lambda>\n  File \"/home/amy/ExtremeWeatherBench/src/extremeweatherbench/evaluate.py\", line 578, in run_pipeline\n    .pipe(input_data.maybe_convert_to_dataset)\n     ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/amy/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/polars/lazyframe/frame.py\", line 942, in pipe\n    return function(self, *args, **kwargs)\n  File \"/home/amy/ExtremeWeatherBench/src/extremeweatherbench/inputs.py\", line 231, in maybe_convert_to_dataset\n    return self._custom_convert_to_dataset(data)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/home/amy/ExtremeWeatherBench/src/extremeweatherbench/inputs.py\", line 566, in _custom_convert_to_dataset\n    if \"surface_air_temperature\" in data.collect_schema().names():\n                                    ~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/amy/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/polars/lazyframe/frame.py\", line 2488, in collect_schema\n    return Schema(self._ldf.collect_schema(), check_dtypes=False)\n                  ~~~~~~~~~~~~~~~~~~~~~~~~^^\npolars.exceptions.ColumnNotFoundError: unable to find column \"valid_time\"; valid columns: [\"surface_air_temperature\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'select' <---\nSELECT [col(\"surface_air_temperature\")]\n  Parquet SCAN [gs://extremeweatherbench/datasets/ghcnh_all_2020_2024.parq]\n  PROJECT */15 COLUMNS\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,return_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m,backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m case_operators_with_targets_established_generator \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m      8\u001b[0m     delayed(\u001b[38;5;28;01mlambda\u001b[39;00m co: (co\u001b[38;5;241m.\u001b[39mcase_metadata\u001b[38;5;241m.\u001b[39mcase_id_number, evaluate\u001b[38;5;241m.\u001b[39mrun_pipeline(co\u001b[38;5;241m.\u001b[39mcase_metadata, co\u001b[38;5;241m.\u001b[39mtarget)))(case_operator) \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m case_operator \u001b[38;5;129;01min\u001b[39;00m case_operators\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m case_operators_with_targets_established \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcase_operators_with_targets_established_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# this will throw a bunch of errors below but they're not consequential. this releases the memory as it shuts down the workers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# now that they're not used\u001b[39;00m\n\u001b[1;32m     14\u001b[0m get_reusable_executor()\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ExtremeWeatherBench/.venv/lib/python3.13/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mColumnNotFoundError\u001b[0m: unable to find column \"valid_time\"; valid columns: [\"surface_air_temperature\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'select' <---\nSELECT [col(\"surface_air_temperature\")]\n  Parquet SCAN [gs://extremeweatherbench/datasets/ghcnh_all_2020_2024.parq]\n  PROJECT */15 COLUMNS"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky import get_reusable_executor\n",
    "# load in all the case info (note this takes awhile in non-parallel form as it has to run all the target information for each case)\n",
    "# this will return a list of tuples with the case id and the target dataset\n",
    "\n",
    "parallel = Parallel(n_jobs=8,return_as='generator',backend='loky')\n",
    "case_operators_with_targets_established_generator = parallel(\n",
    "    delayed(lambda co: (co.case_metadata.case_id_number, evaluate.run_pipeline(co.case_metadata, co.target)))(case_operator) \n",
    "    for case_operator in case_operators\n",
    ")\n",
    "case_operators_with_targets_established = list(case_operators_with_targets_established_generator)\n",
    "# this will throw a bunch of errors below but they're not consequential. this releases the memory as it shuts down the workers\n",
    "# now that they're not used\n",
    "get_reusable_executor().shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in case_operators_with_targets_established:\n",
    "    print(target)\n",
    "    break\n",
    "len(case_operators_with_targets_established)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the indivdual cases for each event type\n",
    "# cp.plot_all_cases_and_obs(ewb_cases, event_type='tropical_cyclone', filename='ewb_obs_tcs.png', targets=case_operators_with_targets_established)\n",
    "# cp.plot_all_cases_and_obs(ewb_cases, event_type='freeze', filename='ewb_obs_freeze.png', targets=case_operators_with_targets_established)\n",
    "# cp.plot_all_cases_and_obs(ewb_cases, event_type='heat_wave', filename='ewb_obs_heat.png', targets=case_operators_with_targets_established)\n",
    "#plot_all_cases_and_obs(ewb_cases, event_type='atmospheric_river', filename=basepath + 'docs/notebooks/figs/ewb_obs_ar.png')\n",
    "cp.plot_all_cases_and_obs(ewb_cases, event_type='severe_convection', filename=basepath + 'docs/notebooks/figs/ewb_obs_convective.png', targets=case_operators_with_targets_established)\n",
    "\n",
    "# # plot all cases on one giant world map\n",
    "# plot_all_cases_and_obs(ewb_cases, event_type=None, filename=basepath + 'docs/notebooks/figs/ewb_obs_all.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install metpy version 1.6.3 for this notebook only\n",
    "!uv pip install metpy==1.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb116c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is modified from the original code for CONUS to work with Australian LSR data\n",
    "def extract_lsr_data_australia(date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Pull the latest LSR data for a given date. We pull all reorts within 1 day. If date is none, we return all reports\n",
    "    \n",
    "    Args:\n",
    "        date: A pandas Timestamp object.\n",
    "    Returns:\n",
    "        df: A pandas DataFrame containing the LSR data with columns lat, lon, report_type, time, and scale.\n",
    "    \"\"\"\n",
    "\n",
    "    aus_file = \"gs://extremeweatherbench/datasets/AustralianLSRData_2020-2024.csv\"\n",
    "\n",
    "    # Read the CSV files with all columns to identify report types\n",
    "    try:\n",
    "        df = pd.read_csv(aus_file, delimiter=',', engine='python', storage_options=dict(token=\"anon\"), parse_dates=['Date/Time UTC'], date_format='%Y-%m-%d %H:%M:%S')\n",
    "    except Exception as e:\n",
    "        print(f'Error pulling hail data for {date}: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # if date is none, presumably we want all reports\n",
    "    if (date == None):\n",
    "        return df\n",
    "\n",
    "    # Filter the DataFrame for the specified date range\n",
    "    start_date = date - pd.Timedelta(days=0.5)    \n",
    "    end_date = date + pd.Timedelta(days=0.5)\n",
    "    df = df[(df['Date/Time UTC'] >= start_date) & (df['Date/Time UTC'] < end_date)]\n",
    "    if len(df) == 0:\n",
    "        print(f'No LSR data found for {date}')\n",
    "        return pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def practically_perfect_hindcast_aus(\n",
    "    date: pd.Timestamp,\n",
    "    resolution: float = 0.25,\n",
    "    report_type: Union[Literal[\"all\"], list[Literal[\"tor\", \"hail\", \"wind\"]]] = \"all\",\n",
    "    sigma: float = 1.5,\n",
    "    return_reports: bool = False,\n",
    "    output_resolution: Optional[float] = None,\n",
    "    report_constants = {'hail': 15, 'tor': 5}\n",
    ") -> Union[xr.DataArray, tuple[xr.DataArray, pd.DataFrame]]:\n",
    "    \"\"\"Compute the Practically Perfect Hindcast (PPH) using storm report data using latitude/longitude grid spacing\n",
    "    instead of the NCEP 212 Eta Lambert Conformal projection; based on the method described in Hitchens et al 2013,\n",
    "    https://doi.org/10.1175/WAF-D-12-00113.1\n",
    "\n",
    "    Args:\n",
    "        date: A pandas Timestamp object.\n",
    "        resolution: The resolution of the grid to use. Default is 0.25 degrees.\n",
    "        report_type: The type of report to use. Default is all. Currently only supports all.\n",
    "        sigma: The sigma (standard deviation) of the gaussian filter to use. Default is 1.5.\n",
    "        return_reports: Whether to return the reports used to compute the PPH. Default is False.\n",
    "        output_resolution: The resolution of the output grid. Default is None (keep the same resolution as the input grid).\n",
    "        report_constants: A dictionary mapping report types to their respective values for the PPH grid. Default is {'hail': 15, 'tor': 5}.\n",
    "    Returns:\n",
    "        pph: An xarray DataArray containing the PPH around the storm report data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = extract_lsr_data_australia(date)\n",
    "    if report_type == \"all\":\n",
    "        pass\n",
    "    else:\n",
    "        df = df[df['report_type'].isin(report_type)]\n",
    "\n",
    "    # Create a grid covering Australia\n",
    "    lat_min, lat_max = -50, -10.0   \n",
    "    lon_min, lon_max = 110, 180  \n",
    "\n",
    "    # Create the grid coordinates\n",
    "    grid_lats = np.arange(lat_min, lat_max + resolution, resolution)\n",
    "    grid_lons = np.arange(lon_min, lon_max + resolution, resolution)\n",
    "\n",
    "    # Initialize an empty grid\n",
    "    grid = np.zeros((len(grid_lats), len(grid_lons)))\n",
    "\n",
    "    # extract reports for TOR and HAIL separately to handle the underreporting\n",
    "    for report_type in report_constants.keys():\n",
    "        # Filter the dataframe for the current report type\n",
    "        df2 = df[df['report_type'] == report_type]\n",
    "\n",
    "        # Extract latitude and longitude from the dataframe\n",
    "        lats = df2[\"Latitude\"].astype(float)\n",
    "        lons = df2[\"Longitude\"].astype(float)\n",
    "\n",
    "        # Mark grid cells that contain reports\n",
    "        for lat, lon in zip(lats, lons):\n",
    "            # Find the nearest grid indices\n",
    "            lat_idx = np.abs(grid_lats - lat).argmin()\n",
    "            lon_idx = np.abs(grid_lons - lon).argmin()\n",
    "            grid[lat_idx, lon_idx] = report_constants[report_type]  # Set a value to indicate a report is present\n",
    "\n",
    "    # Create the xarray DataArray\n",
    "    pph = xr.DataArray(\n",
    "        grid,\n",
    "        dims=[\"latitude\", \"longitude\"],\n",
    "        coords={\"latitude\": grid_lats, \"longitude\": grid_lons},\n",
    "        name=\"practically_perfect\",\n",
    "    )\n",
    "\n",
    "    # Apply bilinear interpolation to smooth the field\n",
    "    # First, create a gaussian kernel for smoothing\n",
    "    smoothed_grid = gaussian_filter(grid, sigma=sigma)\n",
    "\n",
    "    # Replace the data in the DataArray\n",
    "    pph.data = smoothed_grid\n",
    "    if output_resolution is not None:\n",
    "        pph = pph.interp(latitude=np.arange(lat_min, lat_max + output_resolution, output_resolution),\n",
    "                        longitude=np.arange(lon_min, lon_max + output_resolution, output_resolution),method='linear')\n",
    "        pph = pph * 100\n",
    "    if return_reports:\n",
    "        return (pph, df)\n",
    "    return pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93530502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to pre-compute the obs info and store it in the cases\n",
    "# I think there should be either \n",
    "# 1) a quick way to grab all the metadata on the obs into the case (e.g. through a single function call at the high level) or\n",
    "# 2) it is already stored into the events yaml (just the locations, but this isn't super extensible if more obs come in later)\n",
    "# 3) a method call on each individual case to get the metadata\n",
    "# I'm going with option 3 for now\n",
    "\n",
    "# Note since there is about to be a big refactor of how events and cases are stored, this is just a temporary hack\n",
    "events_metadata = {}\n",
    "for event in cases:\n",
    "    for indiv_case in event.cases:\n",
    "        # this is what I want to see for each case\n",
    "        # commented out because it doesn't exist :) \n",
    "        # indiv_case.get_observations_metadata()\n",
    "\n",
    "        # instead I'll try to hack it\n",
    "        if (indiv_case.event_type == 'heat_wave' or indiv_case.event_type == 'freeze'):\n",
    "            # figure our how to find the obs metadata for the heat/freeze events\n",
    "            #print(\"Help, not sure how to find the obs locations\")\n",
    "            continue\n",
    "        elif (indiv_case.event_type == 'severe_convection'):\n",
    "            # grab the LSRs and compute PPH and then we can plot both the LSRs and the PPH outer outline (up in the plotting function)\n",
    "\n",
    "            # check if the case is inside the Australia bounding box\n",
    "            bot_lat = -50\n",
    "            top_lat = -10\n",
    "            left_lon = 110\n",
    "            right_lon = 180\n",
    "            bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "            bounding_box_polygon = cp.get_polygon_from_bounding_box(bounding_box)\n",
    "\n",
    "            if (shapely.intersects(indiv_case.location.geopandas.geometry[0], bounding_box_polygon)):\n",
    "                print(indiv_case)\n",
    "                \n",
    "                pph, df = practically_perfect_hindcast_aus(indiv_case.start_date, return_reports=True)\n",
    "                events_metadata[indiv_case.case_id_number] = {'pph': pph, 'lsr_reports': df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8713325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot North America\n",
    "bot_lat = 7\n",
    "top_lat = 85\n",
    "left_lon = -172\n",
    "right_lon = -45\n",
    "\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "plot_title = 'ExtremeWeatherBench Cases in North America'\n",
    "\n",
    "cp.plot_all_cases_and_obs(ewb_cases, event_type=None, bounding_box=bounding_box, targets=case_operators_with_targets_established)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Europe\n",
    "bot_lat = 15\n",
    "top_lat = 75\n",
    "left_lon = -15\n",
    "right_lon = 50\n",
    "\n",
    "print(right_lon, left_lon, bot_lat, top_lat)\n",
    "\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "plot_title = 'ExtremeWeatherBench Cases in Europe'\n",
    "cp.plot_all_cases_and_obs(ewb_cases, event_type=None, bounding_box=bounding_box, targets=case_operators_with_targets_established)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Australia\n",
    "bot_lat = -50\n",
    "top_lat = -10\n",
    "left_lon = 110\n",
    "right_lon = 180\n",
    "bounding_box = [left_lon, right_lon, bot_lat, top_lat]\n",
    "plot_title = 'ExtremeWeatherBench Cases in Australia'\n",
    "\n",
    "cp.plot_all_cases_and_obs(ewb_cases, event_type=None, bounding_box=bounding_box, targets=case_operators_with_targets_established)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441ebc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
