{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f6f64a7b990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import fsspec\n",
    "from kerchunk.hdf import SingleHdf5ToZarr \n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import ujson\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask\n",
    "dask.config.set({'temporary_directory': '/mnt/disks/data/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('gcs', anon=False, skip_instance_cache=True)\n",
    "fs_local = fsspec.filesystem('')  \n",
    "json_dir = 'assets/json/'\n",
    "json_list = fs_local.glob(str(json_dir)+'PANG*_.json')\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "def convert_longitude_to_360(longitude):\n",
    "    return longitude % 360\n",
    "\n",
    "def generate_json_from_grap_nc(u,fs, fs_out):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "\n",
    "        file_split = u.split('/') # seperate file path to create a unique name for each json \n",
    "        model = file_split[1].split('_')[0]\n",
    "        date_string = file_split[-1].split('_')[3]\n",
    "        outf = f'{json_dir}{model}_{date_string}_.json'\n",
    "        print(outf)\n",
    "        with fs_out.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());\n",
    "\n",
    "# seasonal aggregation functions for max, min, and mean\n",
    "def seasonal_subset_max(df):\n",
    "    df = df.where(df.index.month.isin([6,7,8]))\n",
    "    return df.max()\n",
    "\n",
    "def seasonal_subset_min(df):\n",
    "    df = df.where(df.index.month.isin([6,7,8]))\n",
    "    return df.min()\n",
    "\n",
    "def seasonal_subset_mean(df):\n",
    "    df = df.where(df.index.month.isin([6,7,8]))\n",
    "    return df.mean()\n",
    "\n",
    "def is_jja(month):\n",
    "    return (month >= 6) & (month <= 8)\n",
    "\n",
    "def is_6_hourly(hour):\n",
    "    return (hour == 0) | (hour == 6) | (hour == 12) | (hour == 18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(\n",
    "    n_workers=8,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='6GiB',\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSEA Obs Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksea_event_df = pd.read_csv(f'assets/data/KSEA_event.csv').dropna()\n",
    "ksea_event_df['valid'] = pd.to_datetime(ksea_event_df['valid'])\n",
    "ksea_event_df['tmpc'] = ksea_event_df['tmpc'].astype(float)\n",
    "ksea_event_df = ksea_event_df.set_index('valid')\n",
    "ksea_event_df = ksea_event_df.resample('h').mean(numeric_only=True)\n",
    "ksea_event_df = ksea_event_df[['tmpc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSEA and Other Obs Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_obs_dict = {}\n",
    "point_obs_dict['KSEA'] = {}\n",
    "point_obs_dict['KBLI'] = {}\n",
    "point_obs_dict['CYVR'] = {}\n",
    "point_obs_dict['CWLY'] = {}\n",
    "\n",
    "for station in point_obs_dict.items():\n",
    "    station[1]['data'] = pd.read_csv(f'assets/data/{station[0]}.csv').dropna()\n",
    "    station[1]['data']['valid'] = pd.to_datetime(station[1]['data']['valid'])\n",
    "    station[1]['data']['tmpc'] = station[1]['data']['tmpc'].astype(float)\n",
    "    station[1]['data'] = station[1]['data'].set_index('valid')\n",
    "    station[1]['data'] = station[1]['data'].resample('h').mean(numeric_only=True)\n",
    "    subset_summer_df = station[1]['data']['tmpc'][station[1]['data']['tmpc'].index.month.isin([6,7,8])]\n",
    "    station[1]['max_temp_85th_percentile'] = np.nanpercentile(subset_summer_df.resample('D').max(),85)\n",
    "    station[1]['min_temp_85th_percentile'] = np.nanpercentile(subset_summer_df.resample('D').min(),85)\n",
    "    station[1]['mean_temp_85th_percentile'] = np.nanpercentile(subset_summer_df.resample('D').mean(),85)\n",
    "    station[1]['temp_85th_percentile'] = np.nanpercentile(subset_summer_df,85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Subset ERA5 ARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/1959-2022-full_37-1h-0p25deg-chunk-1.zarr-v2\",\n",
    "    chunks='auto',\n",
    "    storage_options=dict(token='anon'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset 1989-2019 time frame. 2020 onwards is training data (avoid)\n",
    "era5_climatology = era5[['2m_temperature']].sel(time=slice('1989','2019'))\n",
    "\n",
    "# subset JJA and 00, 06, 12, 18z for summer/heat wave criteria to match model outputs\n",
    "era5_subset_jja = era5_climatology.sel(time=is_jja(era5_climatology['time.month']))\n",
    "era5_subset_hours = era5_subset_jja.sel(time=is_6_hourly(era5_subset_jja['time.hour']))\n",
    "\n",
    "# this will take a few minutes, save the subset dataset to a zarr store\n",
    "era5_subset_hours.to_zarr('/home/taylor/data/era5_subset_hours.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_subset = xr.open_zarr(\n",
    "    '/home/taylor/data/era5_subset_hours.zarr',\n",
    "    chunks='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f6ecf658040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era5_6hourly_climatology = era5_subset.groupby('time.hour').mean()\n",
    "\n",
    "# this will take a few minutes as well, save the subset dataset to a zarr store\n",
    "era5_6hourly_climatology.to_zarr('/home/taylor/data/era5_2m_temperature_mean_6hourly_1989-2019.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the 85th percentile of the JJA 00, 06, 12, 18z data globally for 1989-2019\n",
    "era5_85th_percentile = era5_subset.quantile(0.85,dim='time')\n",
    "\n",
    "# this will take a few minutes as well, save the subset dataset to a zarr store\n",
    "era5_85th_percentile.to_zarr('/home/taylor/data/era5_2m_temperature_85th_percentile_1989-2019.zarr', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brightband",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
