{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from distributed import Client\n",
    "import logging\n",
    "import flox  # make sure its available\n",
    "import flox.xarray\n",
    "from tqdm.contrib.itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "# client = Client(n_workers=20, silence_logs=logging.ERROR)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the ERA5 ARCO dataset with reasonable chunks for the VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = xr.open_zarr(\n",
    "    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n",
    "    chunks=None,\n",
    "    storage_options=dict(token='anon'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset down the 30 year dataset and only select 6 hourly intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the time range\n",
    "era5_30yr = era5.sel(time=slice('1989-12-22', '2020-01-10'))\n",
    "era5_30yr = era5_30yr.sel(time=era5_30yr.time.dt.hour.isin([0, 6, 12, 18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create linearly decaying weight DataArray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rolling window parameters\n",
    "half_window_days = 10\n",
    "percentile = 15\n",
    "\n",
    "weights = np.linspace(0,1, half_window_days + 1)\n",
    "weights = np.concatenate([weights, weights[::-1][1:]])\n",
    "weights /= weights.sum()\n",
    "weight_da = xr.DataArray(weights, dims=['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each dataset by year and save to an interim netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f623047b77494de9aa4f791dc4e46d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for year in tqdm(range(1990, 2020)):\n",
    "    # Get data for this year +/- 10 days\n",
    "    start_date = f\"{year-1}-12-22\"  # 10 days before the start of the year\n",
    "    end_date = f\"{year+1}-01-10\"    # 10 days after the end of the year\n",
    "    era5_1yr = era5.sel(time=slice(start_date, end_date))\n",
    "    era5_1yr = era5_1yr.sel(time=era5_1yr.time.dt.hour.isin([0, 6, 12, 18]))\n",
    "    # Slice the data for the current year window\n",
    "    year_data = era5_1yr['2m_temperature'].compute()\n",
    "    \n",
    "    # Create the rolling window with weights for this year's data\n",
    "    hour_group = []\n",
    "    for hour in [0, 6, 12, 18]:\n",
    "        year_rolling = year_data.sel(time=year_data.time.dt.hour==hour).rolling(time=len(weights), center=True)\n",
    "        year_rolling_weighted = year_rolling.construct('window', sliding_window_view_kwargs={\"automatic_rechunk\": True}).dot(weight_da)\n",
    "        year_rolling_weighted = year_rolling_weighted.compute()\n",
    "        hour_group.append(year_rolling_weighted)\n",
    "\n",
    "        \n",
    "    hour_grouped_weighted_ds = xr.concat(hour_group,dim='time')\n",
    "    hour_grouped_weighted_ds.to_netcdf(f\"/home/taylor/data/hour_era5_30yr_rolling_2m_temperature_weighted_{year}.nc\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb34552c245b4ca990229c1aea288314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "percentile = .15\n",
    "climatology_list=[]\n",
    "for doy, hour in product(range(1, 367), [0,6,12,18]):\n",
    "    single_year_list = []\n",
    "    for year in range(1990, 2020):\n",
    "        rolling_weights_ds = xr.open_dataset(f'/home/taylor/data/hour_era5_30yr_rolling_2m_temperature_weighted_{year}.nc')\n",
    "        rolling_weights_ds_hour = rolling_weights_ds.sel(time=rolling_weights_ds.time.dt.hour == hour)\n",
    "        rolling_weights_ds_hour_doy = rolling_weights_ds_hour.sel(time=rolling_weights_ds_hour.time.dt.dayofyear == doy)\n",
    "        rolling_weights_ds_hour_doy_year = rolling_weights_ds_hour_doy.sel(time=rolling_weights_ds_hour_doy.time.dt.year == year)\n",
    "        rolling_weights_ds_hour_doy_year['2m_temperature'] = rolling_weights_ds_hour_doy_year['__xarray_dataarray_variable__']\n",
    "        data_array = rolling_weights_ds_hour_doy_year['2m_temperature']\n",
    "        single_year_list.append(data_array)\n",
    "        del rolling_weights_ds_hour_doy_year, data_array, rolling_weights_ds_hour_doy, rolling_weights_ds_hour, rolling_weights_ds\n",
    "    single_year_list = [n for n in single_year_list if len(n.time) !=0] # remove any years with no data, such as w/ day 366\n",
    "    output = xr.concat(single_year_list, dim='time')\n",
    "    output_quantile = output.groupby([\"time.dayofyear\", \"time.hour\"]).quantile(percentile)\n",
    "    climatology_list.append(output_quantile)\n",
    "    del output, output_quantile\n",
    "\n",
    "climatology = xr.combine_by_coords(climatology_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatology.to_netcdf('era5_freeze_climatology.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
