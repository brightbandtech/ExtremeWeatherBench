{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, List, Optional, Union, Literal\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dacite\n",
    "from scores.continuous import mae, rmse\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from extremeweatherbench import case, metrics, regions, utils\n",
    "import copy\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.1 environment at: /Users/taylor/code/ExtremeWeatherBench/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. An easy? solution to dealing with variable mapping:\n",
    "    - Create a list of the names of each variable used in EWB that users can extend\n",
    "    - Each Observation should have a default mapping; if it's our observation default we will know the variable names coming in. Users will need to know the names of theirs as well.\n",
    "    - Forecasts will be able to access this mapping as well and can be provided in the ExtremeWeatherBench call\n",
    "2. ~~AppliedMetrics will define exactly what observations, observation variables, and forecast variables to use~~\n",
    "    - I think this is wrong now, it should be defined at the Event level.\n",
    "3. Defining the order of orchestration is still an open question. I guess this will come as I wire the components together... We're running each case as its own entity, thus:\n",
    "    - Each case will have multiple metrics that might or might not reuse Observations and Forecasts. Do metrics inside cases define the variables and DerivedVariables? Or should the EventType? Leaning towards EventType. DerivedVariables hold information on what core variables are needed.\n",
    "        1. Observation(s) should be built first which defines the dimensions for Forecasts. **DONE**\n",
    "        2. Observation(s) derived variables should be processed next. **DONE**\n",
    "        3. Forecasts should then be spatiotemporally subset to the Observation(s).\n",
    "        4. Forecasts derived variables should then be processed.\n",
    "        5. Might be worth then temporally subsetting the Observation(s) to the Forecasts\n",
    "        6. Finally, run applied metrics that use the Observation(s) and Forecasts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_longitude_to_180(longitude: float) -> float:\n",
    "    \"\"\"Convert longitude from 0-360 to -180-180 degrees.\"\"\"\n",
    "    return (longitude + 180) % 360 - 180\n",
    "\n",
    "def extract_coordinates_from_sparse_coo(dataset: xr.Dataset, data_var: str = 'report_type') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract latitude and longitude pairs from a sparse COO array without densifying.\n",
    "    \"\"\"\n",
    "    # Get the sparse COO array\n",
    "    sparse_array = dataset[data_var].data\n",
    "    try:\n",
    "        # Get the coordinates from the COO array\n",
    "        coords = sparse_array.coords\n",
    "    except AttributeError:\n",
    "        print(f\"Warning: {data_var} is not a sparse COO array.\")\n",
    "    \n",
    "    # For dimensions (valid_time, latitude, longitude):\n",
    "    # coords[0] = valid_time indices\n",
    "    # coords[1] = latitude indices  \n",
    "    # coords[2] = longitude indices\n",
    "    \n",
    "    # Extract latitude and longitude indices\n",
    "    lat_indices = coords[1]  # Second dimension\n",
    "    lon_indices = coords[2]  # Third dimension\n",
    "    \n",
    "    # Map indices to actual coordinate values\n",
    "    lats = dataset.latitude.values[lat_indices]\n",
    "    lons = dataset.longitude.values[lon_indices]\n",
    "    \n",
    "    # Create DataFrame with unique coordinate pairs\n",
    "    coords_df = pd.DataFrame({\n",
    "        'latitude': lats,\n",
    "        'longitude': lons\n",
    "    }).drop_duplicates()\n",
    "    \n",
    "    return coords_df\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LocationCoords:\n",
    "    latitude: xr.DataArray\n",
    "    longitude: xr.DataArray\n",
    "\n",
    "def practically_perfect_hindcast(\n",
    "    ds: xr.Dataset,\n",
    "    output_bounds: regions.Region,\n",
    "    resolution: float = 0.25,\n",
    "    report_type: Union[Literal[\"all\"], list[Literal[\"tor\", \"hail\", \"wind\"]]] = \"all\",\n",
    "    sigma: float = 1.5,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Compute the Practically Perfect Hindcast (PPH) using storm report data using latitude/longitude grid spacing\n",
    "    instead of the NCEP 212 Eta Lambert Conformal projection; based on the method described in Hitchens et al 2013,\n",
    "    https://doi.org/10.1175/WAF-D-12-00113.1\n",
    "\n",
    "    Args:\n",
    "        ds: An xarray Dataset containing the storm report data as a sparse (COO) array.\n",
    "        output_interpolation: A LocationCoords object containing the latitude and longitude of the output interpolation.\n",
    "        resolution: The resolution of the grid to use. Default is 0.25 degrees.\n",
    "        report_type: The type of report to use. Default is all. Currently only supports all.\n",
    "        sigma: The sigma (standard deviation) of the gaussian filter to use. Default is 1.5.\n",
    "    Returns:\n",
    "        pph: An xarray DataArray containing the PPH around the storm report data.\n",
    "    \"\"\"\n",
    "    coords_df = extract_coordinates_from_sparse_coo(ds)\n",
    "    coords_df['longitude'] = utils.convert_longitude_to_360(coords_df['longitude'])\n",
    "    valid_lats = coords_df['latitude'].values\n",
    "    valid_lons = coords_df['longitude'].values\n",
    "\n",
    "    # Create coordinates from region, not reports, fixed at 0.25 degree intervals\n",
    "    min_lat_fixed = np.ceil(output_bounds.latitude_min * 4) / 4  # Round up to nearest 0.25\n",
    "    max_lat_fixed = np.floor(output_bounds.latitude_max * 4) / 4  # Round down to nearest 0.25\n",
    "    min_lon_fixed = np.ceil(output_bounds.longitude_min * 4) / 4  # Round up to nearest 0.25\n",
    "    max_lon_fixed = np.floor(output_bounds.longitude_max * 4) / 4  # Round down to nearest 0.25\n",
    "\n",
    "    # Create the grid coordinates\n",
    "    grid_lats = np.arange(min_lat_fixed, max_lat_fixed + resolution, resolution)\n",
    "    grid_lons = np.arange(min_lon_fixed, max_lon_fixed + resolution, resolution)\n",
    "\n",
    "    # Initialize an empty grid\n",
    "    grid = np.zeros((len(grid_lats), len(grid_lons)))\n",
    "\n",
    "    # Mark grid cells that contain reports\n",
    "    for lat, lon in zip(valid_lats, valid_lons):\n",
    "        # Skip reports that are outside the grid bounds\n",
    "        if lat < min_lat_fixed or lat > max_lat_fixed or lon < min_lon_fixed or lon > max_lon_fixed:\n",
    "            continue\n",
    "        # Find the nearest grid indices\n",
    "        lat_idx = np.abs(grid_lats - lat).argmin()\n",
    "        lon_idx = np.abs(grid_lons - lon).argmin()\n",
    "        grid[lat_idx, lon_idx] = 1\n",
    "\n",
    "    pph_ds = xr.Dataset(\n",
    "        data_vars={\"reports\": ([\"latitude\", \"longitude\"], grid)},\n",
    "        coords={\"latitude\": grid_lats, \"longitude\": grid_lons},\n",
    "    )\n",
    "\n",
    "    # Apply bilinear interpolation to smooth the field\n",
    "    # First, create a gaussian kernel for smoothing\n",
    "    smoothed_grid = gaussian_filter(grid, sigma=sigma)\n",
    "\n",
    "    # Combine the data into a Dataset\n",
    "    pph_ds['practically_perfect'] = xr.DataArray(smoothed_grid, dims=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    return pph_ds\n",
    "\n",
    "def plot_practically_perfect_hindcast(test_output, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.STATES, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "\n",
    "    try:\n",
    "        # Get reports data\n",
    "        reports = test_output[0].reports\n",
    "    except KeyError as e:\n",
    "        if \"reports\" in str(e):\n",
    "            reports = test_output.reports\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Create practically perfect forecast from reports using Gaussian smoothing\n",
    "    reports_array = reports.values\n",
    "    sigma = 1.5  # Smoothing parameter - adjust as needed\n",
    "    practically_perfect = gaussian_filter(reports_array, sigma=sigma)\n",
    "\n",
    "    # Create contour plot of practically perfect forecast\n",
    "    contour_levels = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    contour = ax.contourf(reports.longitude, reports.latitude, practically_perfect, \n",
    "                         levels=contour_levels, cmap='plasma', alpha=0.7,\n",
    "                         transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax, label='Percentile')\n",
    "\n",
    "    # Also plot the original report locations as small dots\n",
    "    lats, lons = np.where(reports > 0)\n",
    "    lat_values = reports.latitude.values[lats]\n",
    "    lon_values = reports.longitude.values[lons]\n",
    "    ax.scatter(lon_values, lat_values, c='black', s=10, alpha=0.5,\n",
    "               transform=ccrs.PlateCarree(), marker='.')\n",
    "\n",
    "    # Set extent based on the dataset coordinates\n",
    "    lon_min, lon_max = reports.longitude.min().item(), reports.longitude.max().item()\n",
    "    lat_min, lat_max = reports.latitude.min().item(), reports.latitude.max().item()\n",
    "    ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    ax.set_title('Practically Perfect Hindcast from Observation Reports', loc='left')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWB Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EWB_VARIABLES = {\n",
    "    'data_vars': \n",
    "                 [\n",
    "                     'surface_air_temperature',\n",
    "                     'air_pressure_at_mean_sea_level',\n",
    "                     'surface_air_pressure',\n",
    "                     'surface_wind_speed',\n",
    "                     'surface_wind_from_direction',\n",
    "                     'surface_air_temperature',\n",
    "                     'surface_dew_point_temperature',\n",
    "                     'surface_relative_humidity',\n",
    "                     'surface_eastward_wind',\n",
    "                     'surface_northward_wind',\n",
    "                     'accumulated_1_hour_precipitation',\n",
    "                     'pressure_level',\n",
    "                     'air_temperature',\n",
    "                     'dewpoint_temperature',\n",
    "                     'relative_humidity',\n",
    "                     'specific_humidity',\n",
    "                     'geopotential',\n",
    "                     'geopotential_height',\n",
    "                     'potential_temperature',\n",
    "                     'vertical_velocity',\n",
    "                     'eastward_wind',\n",
    "                     'northward_wind',\n",
    "                     ],\n",
    "                     'coords': [\n",
    "                 'valid_time',\n",
    "                 'init_time',\n",
    "                 'lead_time',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'elevation',\n",
    "                 'station_id',\n",
    "                 'station_long_name',\n",
    "                 'case_id'\n",
    "                 ]\n",
    "                 }\n",
    "\n",
    "# Create mapping from EWB variable names to ERA5 variable names\n",
    "ERA5_MAPPING = {\n",
    "    'surface_air_temperature': '2m_temperature',\n",
    "    'surface_dew_point_temperature': '2m_dewpoint_temperature',\n",
    "    'air_pressure_at_mean_sea_level': 'mean_sea_level_pressure',\n",
    "    'surface_air_pressure': 'surface_pressure',\n",
    "    'accumulated_1_hour_precipitation': 'total_precipitation',\n",
    "    'air_temperature': 'temperature',\n",
    "    'relative_humidity': 'relative_humidity',\n",
    "    'specific_humidity': 'specific_humidity',\n",
    "    'geopotential': 'geopotential',\n",
    "    'geopotential_height': 'geopotential_height',\n",
    "    'potential_temperature': 'potential_temperature',\n",
    "    'vertical_velocity': 'vertical_velocity',\n",
    "    'eastward_wind': 'u_component_of_wind',\n",
    "    'northward_wind': 'v_component_of_wind',\n",
    "    'surface_eastward_wind': '10m_u_component_of_wind',\n",
    "    'surface_northward_wind': '10m_v_component_of_wind'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DerivedVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerivedVariable(ABC):\n",
    "    \"\"\"A base class defining the interface for ExtremeWeatherBench derived variables.\n",
    "    \n",
    "    A DerivedVariable is any variable that requires extra computation, not derived in an\n",
    "    observation or forecast raw dataset. Some examples include the practically perfect hindcast,\n",
    "    MLCAPE, IVT, or atmospheric river masks.\n",
    "    \n",
    "    Attributes:\n",
    "        name: The name of the variable.\n",
    "        input_variables: A list of variables that are used to compute the variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, input_variables: List[str]):\n",
    "        self._name = name\n",
    "        self._input_variables = input_variables\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Get the name of the variable.\"\"\"\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def input_variables(self) -> List[str]:\n",
    "        \"\"\"Get the input variables for the variable.\"\"\"\n",
    "        return self._input_variables\n",
    "\n",
    "    def _check_variables(self, data: xr.Dataset, variables: Optional[List[str]] = None) -> List[str]:\n",
    "        \"\"\"Check that the variables are in the dataset.\"\"\"\n",
    "        if variables is None:\n",
    "            variables = list(data.data_vars)\n",
    "        for variable in variables:\n",
    "            if variable not in data.data_vars:\n",
    "                raise ValueError(f\"Variable {variable} not found in dataset.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute(self, case: case.IndividualCase, data: xr.Dataset, variables: Optional[List[str]] = None) -> xr.Dataset:\n",
    "        \"\"\"Compute the variable from the input variables.\"\"\"\n",
    "\n",
    "\n",
    "class PracticallyPerfectHindcast(DerivedVariable):\n",
    "    \"\"\"A derived variable that computes the practically perfect hindcast.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name = \"practically_perfect_hindcast\", input_variables = ['report_type'])\n",
    "\n",
    "    def compute(self, single_case: case.IndividualCase, data: xr.Dataset, variables: Optional[List[str]] = None) -> xr.Dataset:\n",
    "        \"\"\"Compute the practically perfect hindcast.\"\"\"\n",
    "        pph = practically_perfect_hindcast(data[self.input_variables], output_bounds = single_case.location, report_type = ['tor', 'hail'])\n",
    "        return pph\n",
    "    \n",
    "class CravenSignificantSevereParameter(DerivedVariable):\n",
    "    \"\"\"A derived variable that computes the Craven significant severe parameter.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name = \"craven_significant_severe_parameter\", \n",
    "            input_variables = ['air_temperature', \n",
    "                               'dewpoint_temperature',\n",
    "                               'relative_humidity',\n",
    "                               'eastward_wind',\n",
    "                               'northward_wind',\n",
    "                               'surface_eastward_wind',\n",
    "                               'surface_northward_wind'])\n",
    "\n",
    "    def compute(self, case: case.IndividualCase, data: xr.Dataset, variables: Optional[List[str]] = None) -> xr.Dataset:\n",
    "        \"\"\"Compute the Craven significant severe parameter.\"\"\"\n",
    "        cbss_ds = calc.craven_brooks_significant_severe(\n",
    "            case, data[self.input_variables])\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Storage/access options for gridded observation datasets.\n",
    "ARCO_ERA5_FULL_URI = (\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\"\n",
    ")\n",
    "\n",
    "#: Storage/access options for default point observation dataset.\n",
    "DEFAULT_GHCN_URI = \"gs://extremeweatherbench/datasets/ghcnh.parq\"\n",
    "\n",
    "#: Storage/access options for local storm report (LSR) tabular data.\n",
    "LSR_URI = \"gs://extremeweatherbench/datasets/lsr_01012020_04302025.parq\"\n",
    "\n",
    "IBTRACS_URI = \"https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r01/access/csv/ibtracs.ALL.list.v04r01.csv\"  # noqa: E501\n",
    "\n",
    "# type hint for the data input to the observation classes\n",
    "ObservationDataInput = Union[\n",
    "    xr.Dataset, xr.DataArray, pl.LazyFrame, pd.DataFrame, np.ndarray\n",
    "]\n",
    "\n",
    "\n",
    "class Observation(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all observation types.\n",
    "\n",
    "    An Observation is data that acts as the \"truth\" for a case. It can be a gridded dataset,\n",
    "    a point observation dataset, or any other reference dataset. Observations in EWB\n",
    "    are not required to be the same variable as the forecast dataset, but they must be in the\n",
    "    same coordinate system for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    source: str\n",
    "\n",
    "    @abstractmethod\n",
    "    def _open_data_from_source(\n",
    "        self, storage_options: Optional[dict] = None, **kwargs\n",
    "    ) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Open the observation data from the source, opting to avoid loading the entire dataset into memory if possible.\n",
    "\n",
    "        Args:\n",
    "            source: The source of the observation data, which can be a local path or a remote URL.\n",
    "            storage_options: Optional storage options for the source if the source is a remote URL.\n",
    "\n",
    "        Returns:\n",
    "            The observation data with a type determined by the user.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _subset_data_to_case(\n",
    "        self,\n",
    "        data: ObservationDataInput,\n",
    "        case: case.IndividualCase,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Subset the observation data to the case information provided in IndividualCase.\n",
    "\n",
    "        Time information, spatial bounds, and variables are captured in the case metadata\n",
    "        where this method is used to subset.\n",
    "\n",
    "        Args:\n",
    "            data: The observation data to subset, which should be a xarray dataset, xarray dataarray, polars lazyframe,\n",
    "            pandas dataframe, or numpy array.\n",
    "            variables: The variables to include in the observation. Some observations may not have variables, or\n",
    "            only have a singular variable; thus, this is optional.\n",
    "\n",
    "        Returns:\n",
    "            The observation data with the variables subset to the case metadata.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _maybe_convert_to_dataset(self, data: ObservationDataInput, **kwargs) -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Convert the observation data to an xarray dataset if it is not already.\n",
    "\n",
    "        If this method is used prior to _subset_data_to_case, OOM errors are possible\n",
    "        prior to subsetting.\n",
    "\n",
    "        Args:\n",
    "            data: The observation data already run through _subset_data_to_case.\n",
    "\n",
    "        Returns:\n",
    "            The observation data as an xarray dataset.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _maybe_map_variable_names(self, data: ObservationDataInput, variable_mapping: Optional[dict] = None, **kwargs) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Map the variable names to the observation data, if required.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _maybe_derive_variables(\n",
    "        self, data: xr.Dataset, case: case.IndividualCase, variables: list[str | DerivedVariable], **kwargs\n",
    "    ) -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Derive variables from the observation data if any exist in variables.\n",
    "\n",
    "        Args:\n",
    "            data: The observation data already run through _subset_data_to_case.\n",
    "            variables: The variables to derive.\n",
    "\n",
    "        Returns:\n",
    "            The observation data with the derived variables.\n",
    "        \"\"\"\n",
    "        for v in variables:\n",
    "            # there should only be strings or derived variables in the list\n",
    "            if not isinstance(v, str):\n",
    "                if not issubclass(v, DerivedVariable):\n",
    "                    raise ValueError(f\"Expected str or DerivedVariable, got {type(v)}\")\n",
    "                derived_data = v().compute(data=data, single_case=case, variables=variables)\n",
    "                return derived_data\n",
    "        return data\n",
    "\n",
    "    def run_pipeline(\n",
    "        self,\n",
    "        case: case.IndividualCase,\n",
    "        storage_options: Optional[dict] = None,\n",
    "        variables: Optional[list[str | DerivedVariable]] = None,\n",
    "        variable_mapping: dict = {},\n",
    "        **kwargs,\n",
    "    ) -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Shared method for running the observation pipeline.\n",
    "\n",
    "        Args:\n",
    "            source: The source of the observation data, which can be a local path or a remote URL.\n",
    "            storage_options: Optional storage options for the source if the source is a remote URL.\n",
    "            variables: The variables to include in the observation. Some observations may not have variables, or\n",
    "            only have a singular variable; thus, this is optional.\n",
    "            variable_mapping: A dictionary of variable names to map to the observation data.\n",
    "            **kwargs: Additional keyword arguments to pass in as needed.\n",
    "\n",
    "        Returns:\n",
    "            The observation data with a type determined by the user.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Open data and process through pipeline steps\n",
    "        data = (\n",
    "            self._open_data_from_source(\n",
    "                storage_options=storage_options,\n",
    "                **kwargs,\n",
    "            )\n",
    "            .pipe(\n",
    "                self._maybe_map_variable_names, \n",
    "                variable_mapping=variable_mapping,\n",
    "                **kwargs,\n",
    "            )\n",
    "            .pipe(\n",
    "                self._subset_data_to_case,\n",
    "                case=case,\n",
    "                variables=variables,\n",
    "                **kwargs,\n",
    "            )\n",
    "            .pipe(self._maybe_convert_to_dataset, **kwargs)\n",
    "            .pipe(\n",
    "                self._maybe_derive_variables,  \n",
    "                case=case, \n",
    "                variables=variables or [], \n",
    "                **kwargs,\n",
    "                )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "\n",
    "class ERA5(Observation):\n",
    "    \"\"\"\n",
    "    Observation class for ERA5 gridded data.\n",
    "\n",
    "    The easiest approach to using this class\n",
    "    is to use the ARCO ERA5 dataset provided by Google for a source. Otherwise, either a\n",
    "    different zarr source or modifying the _open_data_from_source method to open the data\n",
    "    using another method is required.\n",
    "    \"\"\"\n",
    "\n",
    "    source: str = ARCO_ERA5_FULL_URI\n",
    "\n",
    "    def _open_data_from_source(\n",
    "        self, \n",
    "        storage_options: Optional[dict] = None, \n",
    "        chunks: dict = {'time': 48, 'latitude': 721, 'longitude': 1440},\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "        data = xr.open_zarr(\n",
    "            self.source,\n",
    "            storage_options=storage_options,\n",
    "            chunks=None,\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def _subset_data_to_case(\n",
    "        self,\n",
    "        data: ObservationDataInput,\n",
    "        case: case.IndividualCase,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "\n",
    "        if not isinstance(data, (xr.Dataset, xr.DataArray)):\n",
    "            raise ValueError(f\"Expected xarray Dataset or DataArray, got {type(data)}\")\n",
    "\n",
    "        # subset time first to avoid OOM masking issues\n",
    "        subset_time_data = data.sel(time=slice(case.start_date, case.end_date))\n",
    "\n",
    "        # check that the variables are in the observation data\n",
    "        if variables is not None and any(\n",
    "            var not in subset_time_data.data_vars for var in variables\n",
    "        ):\n",
    "            raise ValueError(f\"Variables {variables} not found in observation data\")\n",
    "        # subset the variables\n",
    "        elif variables is not None:\n",
    "            subset_time_variable_data = subset_time_data[variables]\n",
    "        else:\n",
    "            raise ValueError(\"Variables not defined for ERA5. Please list at least one variable to select.\")\n",
    "        # # calling chunk here to avoid loading subset_data into memory\n",
    "        chunks = kwargs.get('chunks', {'time': 48, 'latitude': 721, 'longitude': 1440})\n",
    "        subset_time_variable_data = subset_time_variable_data.chunk(chunks) \n",
    "        # mask the data to the case location\n",
    "        fully_subset_data = case.location.mask(subset_time_variable_data, drop=True)\n",
    "\n",
    "        return fully_subset_data\n",
    "\n",
    "    def _maybe_convert_to_dataset(self, data: ObservationDataInput, **kwargs):\n",
    "        if isinstance(data, xr.DataArray):\n",
    "            data = data.to_dataset()\n",
    "        return data\n",
    "\n",
    "    def _maybe_map_variable_names(self, data: ObservationDataInput, variable_mapping: Optional[dict] = None, **kwargs) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Map the variable names to the observation data, if required.\n",
    "        \"\"\"\n",
    "        if variable_mapping is None:\n",
    "            return data\n",
    "        # Filter the mapping to only include variables that exist in the dataset\n",
    "        filtered_mapping = {v: k for k, v in variable_mapping.items() if v in data.data_vars}\n",
    "        if filtered_mapping:\n",
    "            data = data.rename(filtered_mapping)\n",
    "        return data\n",
    "\n",
    "\n",
    "class GHCN(Observation):\n",
    "    \"\"\"\n",
    "    Observation class for GHCN tabular data.\n",
    "\n",
    "    Data is processed using polars to maintain the lazy loading\n",
    "    paradigm in _open_data_from_source and to separate the subsetting\n",
    "    into _subset_data_to_case.\n",
    "    \"\"\"\n",
    "\n",
    "    source: str = DEFAULT_GHCN_URI\n",
    "\n",
    "    def _open_data_from_source(\n",
    "        self, storage_options: Optional[dict] = None, **kwargs\n",
    "    ) -> ObservationDataInput:\n",
    "        observation_data: pl.LazyFrame = pl.scan_parquet(\n",
    "            self.source, storage_options=storage_options\n",
    "        )\n",
    "\n",
    "        return observation_data\n",
    "\n",
    "    def _subset_data_to_case(\n",
    "        self,\n",
    "        observation_data: ObservationDataInput,\n",
    "        case: case.IndividualCase,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "        # Create filter expressions for LazyFrame\n",
    "        time_min = case.start_date - pd.Timedelta(days=2)\n",
    "        time_max = case.end_date + pd.Timedelta(days=2)\n",
    "\n",
    "        if not isinstance(observation_data, pl.LazyFrame):\n",
    "            raise ValueError(f\"Expected polars LazyFrame, got {type(observation_data)}\")\n",
    "\n",
    "        # Apply filters using proper polars expressions\n",
    "        subset_observation_data = observation_data.filter(\n",
    "            (pl.col(\"time\") >= time_min)\n",
    "            & (pl.col(\"time\") <= time_max)\n",
    "            & (pl.col(\"latitude\") >= case.location.latitude_min)\n",
    "            & (pl.col(\"latitude\") <= case.location.latitude_max)\n",
    "            & (pl.col(\"longitude\") >= case.location.longitude_min)\n",
    "            & (pl.col(\"longitude\") <= case.location.longitude_max)\n",
    "        )\n",
    "\n",
    "        # Add time, latitude, and longitude to the variables, polars doesn't do indexes\n",
    "        if variables is None:\n",
    "            all_variables = [\"time\", \"latitude\", \"longitude\"]\n",
    "        else:\n",
    "            all_variables = variables + [\"time\", \"latitude\", \"longitude\"]\n",
    "\n",
    "        # check that the variables are in the observation data\n",
    "        schema_fields = [field for field in subset_observation_data.collect_schema()]\n",
    "        if variables is not None and any(\n",
    "            var not in schema_fields for var in all_variables\n",
    "        ):\n",
    "            raise ValueError(f\"Variables {all_variables} not found in observation data\")\n",
    "\n",
    "        # subset the variables\n",
    "        if variables is not None:\n",
    "            subset_observation_data = subset_observation_data.select(all_variables)\n",
    "\n",
    "        return subset_observation_data\n",
    "\n",
    "    def _maybe_convert_to_dataset(self, data: ObservationDataInput, **kwargs):\n",
    "        if isinstance(data, pl.LazyFrame):\n",
    "            data = data.collect().to_pandas()\n",
    "            data = data.set_index([\"time\", \"latitude\", \"longitude\"])\n",
    "            # GHCN data can have duplicate values right now, dropping here if it occurs\n",
    "            try:\n",
    "                data = data.to_xarray()\n",
    "            except ValueError as e:\n",
    "                if \"non-unique\" in str(e):\n",
    "                    pass\n",
    "                data = data.drop_duplicates().to_xarray()\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(f\"Data is not a polars LazyFrame: {type(data)}\")\n",
    "        \n",
    "    def _maybe_map_variable_names(self, data: ObservationDataInput, variable_mapping: dict, **kwargs) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Map the variable names to the observation data, if required.\n",
    "        \"\"\"\n",
    "        # Filter the mapping to only include variables that exist in the dataset\n",
    "        filtered_mapping = {v: k for k, v in variable_mapping.items() if v in data.columns}\n",
    "        if filtered_mapping:\n",
    "            data = data.rename(filtered_mapping)\n",
    "        return data\n",
    "\n",
    "class LSR(Observation):\n",
    "    \"\"\"\n",
    "    Observation class for local storm report (LSR) tabular data.\n",
    "\n",
    "    run_pipeline() returns a dataset with LSRs and practically perfect hindcast gridded\n",
    "    probability data. IndividualCase date ranges for LSRs should ideally be\n",
    "    12 UTC to the next day at 12 UTC to match SPC methods.\n",
    "    \"\"\"\n",
    "\n",
    "    source: str = LSR_URI\n",
    "\n",
    "    def _open_data_from_source(\n",
    "        self, storage_options: Optional[dict] = None, **kwargs\n",
    "    ) -> ObservationDataInput:\n",
    "        \n",
    "        # force LSR to use anon token to prevent google reauth issues for users\n",
    "        observation_data = pd.read_parquet(self.source, storage_options={'token': 'anon'})\n",
    "\n",
    "        return observation_data\n",
    "\n",
    "    def _subset_data_to_case(\n",
    "        self,\n",
    "        observation_data: ObservationDataInput,\n",
    "        case: case.IndividualCase,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "        if not isinstance(observation_data, pd.DataFrame):\n",
    "            raise ValueError(f\"Expected pandas DataFrame, got {type(observation_data)}\")\n",
    "\n",
    "        # latitude, longitude are strings by default, convert to float\n",
    "        observation_data[\"lat\"] = observation_data[\"lat\"].astype(float)\n",
    "        observation_data[\"lon\"] = observation_data[\"lon\"].astype(float)\n",
    "        observation_data[\"time\"] = pd.to_datetime(observation_data[\"time\"])\n",
    "\n",
    "        filters = (\n",
    "            (observation_data[\"time\"] >= case.start_date)\n",
    "            & (observation_data[\"time\"] <= case.end_date)\n",
    "            & (observation_data[\"lat\"] >= case.location.latitude_min)\n",
    "            & (observation_data[\"lat\"] <= case.location.latitude_max)\n",
    "            & (observation_data[\"lon\"] >= convert_longitude_to_180(case.location.longitude_min))\n",
    "            & (observation_data[\"lon\"] <= convert_longitude_to_180(case.location.longitude_max))\n",
    "        )\n",
    "\n",
    "        subset_observation_data = observation_data.loc[filters]\n",
    "\n",
    "        subset_observation_data = subset_observation_data.rename(\n",
    "            columns={\"lat\": \"latitude\", \"lon\": \"longitude\", \"time\": \"valid_time\"}\n",
    "        )\n",
    "\n",
    "        return subset_observation_data\n",
    "\n",
    "    def _maybe_convert_to_dataset(self, data: ObservationDataInput, **kwargs):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.set_index([\"valid_time\", \"latitude\", \"longitude\"])\n",
    "            data = xr.Dataset.from_dataframe(data[~data.index.duplicated(keep='first')],sparse=True)\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(f\"Data is not a pandas DataFrame: {type(data)}\")\n",
    "\n",
    "    def _maybe_map_variable_names(self, data: ObservationDataInput, variable_mapping: dict, **kwargs) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Map the variable names to the observation data, if required.\n",
    "        \"\"\"\n",
    "        # Filter the mapping to only include variables that exist in the dataset\n",
    "        filtered_mapping = {v: k for k, v in variable_mapping.items() if v in data.columns}\n",
    "        if filtered_mapping:\n",
    "            data = data.rename(filtered_mapping)\n",
    "        return data\n",
    "\n",
    "class IBTrACS(Observation):\n",
    "    \"\"\"\n",
    "    Observation class for IBTrACS data.\n",
    "    \"\"\"\n",
    "\n",
    "    source: str = IBTRACS_URI\n",
    "\n",
    "    def _open_data_from_source(\n",
    "        self, storage_options: Optional[dict] = None, **kwargs\n",
    "    ) -> ObservationDataInput:\n",
    "        # not using storage_options in this case due to NetCDF4Backend not supporting them\n",
    "        observation_data: pl.LazyFrame = pl.scan_csv(\n",
    "            self.source, storage_options=storage_options\n",
    "        )\n",
    "        return observation_data\n",
    "\n",
    "    def _subset_data_to_case(\n",
    "        self,\n",
    "        observation_data: ObservationDataInput,\n",
    "        case: case.IndividualCase,\n",
    "        variables: Optional[list[str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> ObservationDataInput:\n",
    "        # Create filter expressions for LazyFrame\n",
    "        year = case.start_date.year\n",
    "\n",
    "        if not isinstance(observation_data, pl.LazyFrame):\n",
    "            raise ValueError(f\"Expected polars LazyFrame, got {type(observation_data)}\")\n",
    "\n",
    "        # Apply filters using proper polars expressions\n",
    "        subset_observation_data = observation_data.filter(\n",
    "            (pl.col(\"NAME\") == case.title.upper())\n",
    "        )\n",
    "\n",
    "        all_variables = [\n",
    "            \"SEASON\",\n",
    "            \"NUMBER\",\n",
    "            \"NAME\",\n",
    "            \"ISO_TIME\",\n",
    "            \"LAT\",\n",
    "            \"LON\",\n",
    "            \"WMO_WIND\",\n",
    "            \"USA_WIND\",\n",
    "            \"WMO_PRES\",\n",
    "            \"USA_PRES\",\n",
    "        ]\n",
    "        # Get the season (year) from the case start date, cast as string as polars is interpreting the schema as strings\n",
    "        season = str(year)\n",
    "\n",
    "        # First filter by name to get the storm data\n",
    "        subset_observation_data = observation_data.filter(\n",
    "            (pl.col(\"NAME\") == case.title.upper())\n",
    "        )\n",
    "\n",
    "        # Create a subquery to find all storm numbers in the same season\n",
    "        matching_numbers = (\n",
    "            subset_observation_data.filter(pl.col(\"SEASON\") == season)\n",
    "            .select(\"NUMBER\")\n",
    "            .unique()\n",
    "        )\n",
    "\n",
    "        # Apply the filter to get all data for storms with the same number in the same season\n",
    "        # This maintains the lazy evaluation\n",
    "        subset_observation_data = observation_data.join(\n",
    "            matching_numbers, on=\"NUMBER\", how=\"inner\"\n",
    "        ).filter((pl.col(\"NAME\") == case.title.upper()) & (pl.col(\"SEASON\") == season))\n",
    "\n",
    "        # check that the variables are in the observation data\n",
    "        schema_fields = [field for field in subset_observation_data.collect_schema()]\n",
    "        if variables is not None and any(\n",
    "            var not in schema_fields for var in all_variables\n",
    "        ):\n",
    "            raise ValueError(f\"Variables {all_variables} not found in observation data\")\n",
    "\n",
    "        # subset the variables\n",
    "        if variables is not None:\n",
    "            subset_observation_data = subset_observation_data.select(all_variables)\n",
    "\n",
    "        return subset_observation_data\n",
    "\n",
    "    def _maybe_convert_to_dataset(self, data: ObservationDataInput, **kwargs):\n",
    "        if isinstance(data, pl.LazyFrame):\n",
    "            data = data.collect().to_pandas()\n",
    "            data = data.set_index([\"ISO_TIME\"])\n",
    "            try:\n",
    "                data = data.to_xarray()\n",
    "            except ValueError as e:\n",
    "                if \"non-unique\" in str(e):\n",
    "                    pass\n",
    "                data = data.drop_duplicates().to_xarray()\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(f\"Data is not a polars LazyFrame: {type(data)}\")\n",
    "        \n",
    "    def _maybe_map_variable_names(self, data: ObservationDataInput, variable_mapping: dict, **kwargs) -> ObservationDataInput:\n",
    "        \"\"\"\n",
    "        Map the variable names to the observation data, if required.\n",
    "        \"\"\"\n",
    "        # Filter the mapping to only include variables that exist in the dataset\n",
    "        filtered_mapping = {v: k for k, v in variable_mapping.items() if v in data.columns}\n",
    "        if filtered_mapping:\n",
    "            data = data.rename(filtered_mapping)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement this in case.py\n",
    "@dataclasses.dataclass\n",
    "class BaseCaseMetadataCollection:\n",
    "    cases: List[case.IndividualCase]\n",
    "\n",
    "    def subset_cases_by_event_type(self, event_type: str) -> List[case.IndividualCase]:\n",
    "        \"\"\"Subset the cases in the collection by event type.\"\"\"\n",
    "        return [c for c in self.cases if c.event_type == event_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class CaseOperator:\n",
    "    \"\"\"A class which stores the graph to process an individual case.\"\"\"\n",
    "    \n",
    "    case: case.IndividualCase\n",
    "    metrics: list[metrics.Metric]\n",
    "    observations: list[Observation]\n",
    "    \n",
    "    def evaluate_case(self, forecast: xr.Dataset):\n",
    "        \"\"\"Process a case.\"\"\"\n",
    "        self.process_metrics(forecast)\n",
    "        \n",
    "    def process_metrics(self, forecast: xr.Dataset):\n",
    "        \"\"\"Process the metrics.\"\"\"\n",
    "        for metric in self.metrics:\n",
    "            metric.process_metric(forecast, self.observations)\n",
    "\n",
    "    def build_observations(self) -> xr.Dataset:\n",
    "        \"\"\"Build observation xarray Datasets from the observation sources.\"\"\"\n",
    "        observation_datasets = []\n",
    "        for observation in self.observation_sources:\n",
    "            obs_dataset = observation.run_pipeline()\n",
    "            observation_datasets.append(obs_dataset)\n",
    "        \n",
    "        # Combine all observation datasets into a single dataset\n",
    "        if len(observation_datasets) >= 1:\n",
    "            combined_obs = xr.merge(observation_datasets)\n",
    "            return combined_obs\n",
    "        else:\n",
    "            raise ValueError(\"No observations provided or observations failed to run, check the observation sources.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics:\n",
    "\n",
    "It seems logical to split up simple metrics, e.g. MAE, and more complicated split-apply-combine or other methods that requires multiple steps to prepare for said simple metric. These more complicated metrics are \"AppliedMetrics\", and can optionally include the simple metrics. Some metrics part of EWB don't have a simple metric downstream, such as categorical thresholds and contingency table metrics. This is a bit of a WIP so will update as the orchestration becomes more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMetric(ABC):\n",
    "    @abstractmethod\n",
    "    def compute(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        pass\n",
    "\n",
    "class AppliedMetric(ABC):\n",
    "    def __init__(\n",
    "            self, \n",
    "            metric: BaseMetric, \n",
    "            observation_sources: list[Observation],\n",
    "            variables: list[str | DerivedVariable],\n",
    "            \n",
    "            ):\n",
    "        self.metric = metric\n",
    "        self.observation_sources = observation_sources\n",
    "        self.variables = variables\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset):\n",
    "        return self.metric.compute(forecast, self.observation_sources)\n",
    "\n",
    "\n",
    "class MAE(BaseMetric):\n",
    "\n",
    "    def compute(self, forecast: xr.Dataset, observation: xr.Dataset, **kwargs):\n",
    "        return mae(forecast, observation, **kwargs)\n",
    "\n",
    "class RMSE(BaseMetric):\n",
    "    def compute(self, forecast: xr.Dataset, observation: xr.Dataset, **kwargs):\n",
    "        return rmse(forecast, observation, **kwargs)\n",
    "\n",
    "class MaximumMAE(AppliedMetric):\n",
    "\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        maximum_timestep = observation.mean([\"latitude\", \"longitude\"]).idxmax(\"valid_time\").values\n",
    "        maximum_value = observation.mean([\"latitude\", \"longitude\"]).sel(valid_time=maximum_timestep).values\n",
    "        forecast_spatial_mean = forecast.mean([\"latitude\", \"longitude\"])\n",
    "        filtered_max_forecast = forecast_spatial_mean.mean(['latitude','longitude']).where(\n",
    "            (forecast_spatial_mean.valid_time >= maximum_timestep - np.timedelta64(48, 'h')) & \n",
    "            (forecast_spatial_mean.valid_time <= maximum_timestep + np.timedelta64(48, 'h')),\n",
    "            drop=True\n",
    "        ).max('valid_time')\n",
    "        return self.metric().compute(filtered_max_forecast, maximum_value)\n",
    "    \n",
    "class RegionalRMSE(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = RMSE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        return self.metric.compute(forecast, observation, preserve_dims='lead_time')\n",
    "    \n",
    "# Dummy metric classes for different event types\n",
    "\n",
    "class MaxMinMAE(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for finding both max and min values\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class OnsetME(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for onset mean error\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class DurationME(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for duration mean error\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class CSI(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for Critical Success Index\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class LeadTimeDetection(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for lead time detection\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class RegionalHitsMisses(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for regional hits and misses\n",
    "        return self.metric().compute(forecast, observation)\n",
    "\n",
    "class HitsMisses(AppliedMetric):\n",
    "    def __init__(self, metric: BaseMetric = MAE, observation: Observation = ERA5):\n",
    "        super().__init__(metric, observation)\n",
    "\n",
    "    def compute_metric(self, forecast: xr.Dataset, observation: xr.Dataset):\n",
    "        # Dummy implementation for hits and misses\n",
    "        return self.metric().compute(forecast, observation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_expand_variable_lists(variable_list: List[str | DerivedVariable]) -> List[str]:\n",
    "    \"\"\"Build a list of core variables for the event, given the forecast and observation variables.\"\"\"\n",
    "\n",
    "    def iterator(variables: List[str | DerivedVariable]) -> List[str]:\n",
    "        for variable in variables:\n",
    "            if isinstance(variable, str):\n",
    "                pass\n",
    "            elif issubclass(variable, DerivedVariable):\n",
    "                variables.extend([n for n in variable().input_variables])\n",
    "        return variables\n",
    "\n",
    "    return iterator(variable_list)\n",
    "\n",
    "class EventType(ABC):\n",
    "    \"\"\"A base class defining the interface for ExtremeWeatherBench event types.\n",
    "\n",
    "    An Event in ExtremeWeatherBench defines a specific weather event type, such as a heat wave,\n",
    "    severe convective weather, or atmospheric rivers. These events encapsulate a set of cases and\n",
    "    derived behavior for evaluating those cases. These cases will share common metrics, observations,\n",
    "    and variables while each having unique dates and locations.\n",
    "\n",
    "    Attributes:\n",
    "        event_type: The type of event.\n",
    "        forecast_variables: A list of variables that are used to forecast the event.\n",
    "        observation_variables: A list of variables that are used to observe the event.\n",
    "        case_metadata: A dictionary or yaml file with guiding metadata.\n",
    "        metrics: A list of Metrics that are used to evaluate the cases.\n",
    "        observations: A list of Observations that are used as targets for the metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        event_type: str,\n",
    "        forecast_variables: List[str | DerivedVariable],\n",
    "        observation_variables: List[str | DerivedVariable],\n",
    "        case_metadata: dict[str, Any],\n",
    "        metrics: List[metrics.Metric],\n",
    "        observations: List[Observation],\n",
    "    ):\n",
    "        self.event_type = event_type\n",
    "        self.forecast_variables = maybe_expand_variable_lists(forecast_variables)\n",
    "        self.observation_variables = maybe_expand_variable_lists(observation_variables)\n",
    "        self.case_metadata = case_metadata\n",
    "        self.metrics = metrics\n",
    "        self.observations = observations\n",
    "\n",
    "    def _build_base_case_metadata_collection(self) -> BaseCaseMetadataCollection:\n",
    "        \"\"\"Build a list of IndividualCases from the case_metadata.\"\"\"\n",
    "        cases = dacite.from_dict(\n",
    "            data_class=BaseCaseMetadataCollection, \n",
    "            data=self.case_metadata, \n",
    "            config=dacite.Config(\n",
    "                    type_hooks={regions.Region: regions.map_to_create_region},\n",
    "                ),\n",
    "        )\n",
    "        cases = BaseCaseMetadataCollection(cases=[c for c in cases.cases if c.event_type == self.event_type])\n",
    "        return cases\n",
    "    \n",
    "    def build_case_operator(self) -> list[CaseOperator]:\n",
    "        \"\"\"Build a CaseOperator from the event type.\"\"\"\n",
    "        case_metadata_collection = self._build_base_case_metadata_collection()\n",
    "        case_operators = [\n",
    "            CaseOperator(\n",
    "                case = case,\n",
    "                metrics = self.metrics,\n",
    "                observations = self.observations,\n",
    "                ) \n",
    "                for case in case_metadata_collection.cases\n",
    "                ]\n",
    "        return case_operators\n",
    "\n",
    "class HeatWave(EventType):\n",
    "    def __init__(self, case_metadata: dict[str, Any], \n",
    "                 forecast_variables: List[str | DerivedVariable] = ['surface_air_temperature'],\n",
    "                 observation_variables: List[str | DerivedVariable] = ['surface_air_temperature'],\n",
    "                 metrics: List[metrics.Metric] = [MaximumMAE,\n",
    "                                                  MaxMinMAE, \n",
    "                                                  RegionalRMSE,\n",
    "                                                  OnsetME,\n",
    "                                                  DurationME], \n",
    "                 observations: List[Observation] = [ERA5]\n",
    "                 ):\n",
    "        super().__init__(event_type='heat_wave', \n",
    "                         forecast_variables=forecast_variables,\n",
    "                         observation_variables=observation_variables,\n",
    "                         case_metadata=case_metadata, \n",
    "                         metrics=metrics, \n",
    "                         observations=observations)\n",
    "\n",
    "class SevereConvection(EventType):\n",
    "    def __init__(self, case_metadata: dict[str, Any], \n",
    "                 forecast_variables: List[str | DerivedVariable] = [\n",
    "                     CravenSignificantSevereParameter,\n",
    "                 ],\n",
    "                 observation_variables: List[str | DerivedVariable] = [\n",
    "                     PracticallyPerfectHindcast,\n",
    "                 ],\n",
    "                 metrics: List[metrics.Metric] = [CSI, \n",
    "                                                  LeadTimeDetection, \n",
    "                                                  RegionalHitsMisses, \n",
    "                                                  HitsMisses\n",
    "                                                  ], \n",
    "                 observations: List[Observation] = [LSR]\n",
    "                 ):\n",
    "        super().__init__(event_type='severe_convection',\n",
    "                         forecast_variables=forecast_variables,\n",
    "                         observation_variables=observation_variables,\n",
    "                         case_metadata=case_metadata, \n",
    "                         metrics=metrics, \n",
    "                         observations=observations)\n",
    "\n",
    "class AtmosphericRiver(EventType):\n",
    "    def __init__(self, case_metadata: dict[str, Any], \n",
    "                 forecast_variables: List[str | DerivedVariable] = [],\n",
    "                 observation_variables: List[str | DerivedVariable] = [],\n",
    "                 metrics: List[metrics.Metric] = [CSI, \n",
    "                                                  LeadTimeDetection,\n",
    "                                                  ], \n",
    "                 observations: List[Observation] = [ERA5]\n",
    "                 ):\n",
    "        super().__init__(event_type='atmospheric_river', \n",
    "                         forecast_variables=forecast_variables,\n",
    "                         observation_variables=observation_variables,\n",
    "                         case_metadata=case_metadata, \n",
    "                         metrics=metrics, \n",
    "                         observations=observations)\n",
    "\n",
    "@dataclasses.dataclass    \n",
    "class EventOperator:\n",
    "    events: List[EventType]\n",
    "    pre_composed_metrics: dict[EventType, List[metrics.Metric]] = dataclasses.field(default_factory=list, init=False, repr=True)\n",
    "    pre_composed_observations: dict[EventType, List[Observation]] = dataclasses.field(default_factory=list, init=False, repr=True)\n",
    "    pre_composed_forecast_variables: dict[EventType, List[str | DerivedVariable]] = dataclasses.field(default_factory=list, init=False, repr=True)\n",
    "    pre_composed_observation_variables: dict[EventType, List[str | DerivedVariable]] = dataclasses.field(default_factory=list, init=False, repr=True)\n",
    "    pre_composed_case_operators: List[CaseOperator] = dataclasses.field(default_factory=list, init=False, repr=True)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Unravel attributes from composed event types\n",
    "        self.pre_composed_metrics = {}\n",
    "        self.pre_composed_observations = {}\n",
    "        self.pre_composed_forecast_variables = {}\n",
    "        self.pre_composed_observation_variables = {}\n",
    "        self.pre_composed_case_operators = []\n",
    "        \n",
    "        # Collect attributes from each event type\n",
    "        for event in self.events:\n",
    "            self.pre_composed_metrics[event.event_type] = event.metrics\n",
    "            self.pre_composed_observations[event.event_type] = event.observations\n",
    "            self.pre_composed_forecast_variables[event.event_type] = event.forecast_variables\n",
    "            self.pre_composed_observation_variables[event.event_type] = event.observation_variables\n",
    "            self.pre_composed_case_operators.extend(event.build_case_operator())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_observations(case_operator: CaseOperator, variables: dict[str, list[str | DerivedVariable]], copy_data = True, **kwargs):\n",
    "    event_type = case_operator.case.event_type\n",
    "    observation_data_list = []\n",
    "    if copy_data:\n",
    "        modified_case_operator = copy.deepcopy(case_operator)\n",
    "    else:\n",
    "        modified_case_operator = case_operator\n",
    "    for observation in modified_case_operator.observations:\n",
    "        observation_data = observation().run_pipeline(case=modified_case_operator.case, variables=variables[event_type], **kwargs)\n",
    "        observation_data_list.append(observation_data)\n",
    "    return observation_data_list\n",
    "\n",
    "class ExtremeWeatherBench:\n",
    "    def __init__(self, event_operator: EventOperator, forecast_dir: str):\n",
    "        self.event_operator = copy.deepcopy(event_operator)\n",
    "        self.forecast_dir = forecast_dir\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        '''Runs the workflow'''\n",
    "        \n",
    "        for case_operator in self.event_operator.pre_composed_case_operators:\n",
    "            print(self.process_case(case_operator, **kwargs))\n",
    "        pass\n",
    "\n",
    "    def process_case(self, case_operator: CaseOperator, **kwargs):\n",
    "        observation_ds = self.process_observations(case_operator, **kwargs)\n",
    "        return observation_ds\n",
    "\n",
    "    def process_observations(self, case_operator: CaseOperator, **kwargs):\n",
    "        pre_derived_observation_ds = _process_observations(case_operator, variables=self.event_operator.pre_composed_observation_variables, **kwargs)\n",
    "        return pre_derived_observation_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Event Operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_yaml = utils.read_event_yaml('/Users/taylor/code/ExtremeWeatherBench/src/extremeweatherbench/data/events.yaml')\n",
    "forecast_dir = 'gs://extremeweatherbench/virtualizarr/fcn_v3.parq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_waves = HeatWave(case_metadata=case_yaml)\n",
    "severe = SevereConvection(case_metadata=case_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the EventOperator class\n",
    "simple_event_operator = EventOperator(events=[heat_waves, severe])\n",
    "ewb = ExtremeWeatherBench(simple_event_operator, forecast_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventOperator(events=[<__main__.HeatWave object at 0x1198d6650>, <__main__.SevereConvection object at 0x1198d69e0>], pre_composed_metrics={'heat_wave': [<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], 'severe_convection': [<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>]}, pre_composed_observations={'heat_wave': [<class '__main__.ERA5'>], 'severe_convection': [<class '__main__.LSR'>]}, pre_composed_forecast_variables={'heat_wave': ['surface_air_temperature'], 'severe_convection': [<class '__main__.CravenSignificantSevereParameter'>, 'air_temperature', 'dewpoint_temperature', 'relative_humidity', 'eastward_wind', 'northward_wind', 'surface_eastward_wind', 'surface_northward_wind', 'air_temperature', 'dewpoint_temperature', 'relative_humidity', 'eastward_wind', 'northward_wind', 'surface_eastward_wind', 'surface_northward_wind']}, pre_composed_observation_variables={'heat_wave': ['surface_air_temperature'], 'severe_convection': [<class '__main__.PracticallyPerfectHindcast'>, 'report_type', 'report_type']}, pre_composed_case_operators=[CaseOperator(case=IndividualCase(case_id_number=1, title='2021 Pacific Northwest', start_date=datetime.datetime(2021, 6, 20, 0, 0), end_date=datetime.datetime(2021, 7, 3, 0, 0), location=CenteredRegion(latitude=47.6062, longitude=237.6679, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=2, title='2022 Upper Midwest', start_date=datetime.datetime(2022, 5, 7, 0, 0), end_date=datetime.datetime(2022, 5, 17, 0, 0), location=CenteredRegion(latitude=41.8781, longitude=272.3702, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=3, title='2022 California', start_date=datetime.datetime(2022, 6, 7, 0, 0), end_date=datetime.datetime(2022, 6, 15, 0, 0), location=CenteredRegion(latitude=34.0522, longitude=241.7563, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=4, title='2022 Texas', start_date=datetime.datetime(2022, 6, 30, 0, 0), end_date=datetime.datetime(2022, 7, 18, 0, 0), location=CenteredRegion(latitude=32.7767, longitude=263.203, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=5, title='2023 Pacific Northwest', start_date=datetime.datetime(2023, 5, 10, 0, 0), end_date=datetime.datetime(2023, 5, 23, 0, 0), location=CenteredRegion(latitude=47.6062, longitude=237.6679, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=6, title='2022 Mid-Atlantic', start_date=datetime.datetime(2022, 5, 17, 0, 0), end_date=datetime.datetime(2022, 5, 24, 0, 0), location=CenteredRegion(latitude=39.2904, longitude=283.38779999999997, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=7, title='2023 Australia', start_date=datetime.datetime(2023, 11, 18, 0, 0), end_date=datetime.datetime(2023, 11, 28, 0, 0), location=CenteredRegion(latitude=-31.9505, longitude=115.8605, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=8, title='2023 Ireland', start_date=datetime.datetime(2023, 9, 2, 0, 0), end_date=datetime.datetime(2023, 9, 13, 0, 0), location=CenteredRegion(latitude=53.1424, longitude=352.3079, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=9, title='2023 Italy', start_date=datetime.datetime(2023, 7, 7, 0, 0), end_date=datetime.datetime(2023, 7, 27, 0, 0), location=CenteredRegion(latitude=41.9028, longitude=12.4964, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=10, title='2023 SW Europe', start_date=datetime.datetime(2023, 8, 17, 0, 0), end_date=datetime.datetime(2023, 8, 28, 0, 0), location=CenteredRegion(latitude=40.4637, longitude=356.2508, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=11, title='2023 South America', start_date=datetime.datetime(2023, 7, 29, 0, 0), end_date=datetime.datetime(2023, 8, 4, 0, 0), location=CenteredRegion(latitude=-34.6037, longitude=301.6184, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=12, title='2023 China (Shanghai)', start_date=datetime.datetime(2023, 5, 24, 0, 0), end_date=datetime.datetime(2023, 6, 1, 0, 0), location=CenteredRegion(latitude=31.2304, longitude=121.4737, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=13, title='2023 China (Yunnan Province)', start_date=datetime.datetime(2023, 4, 14, 0, 0), end_date=datetime.datetime(2023, 4, 23, 0, 0), location=CenteredRegion(latitude=25.0458, longitude=102.71, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=14, title='2023 Algeria', start_date=datetime.datetime(2023, 7, 5, 0, 0), end_date=datetime.datetime(2023, 7, 27, 0, 0), location=CenteredRegion(latitude=36.7372, longitude=3.0869, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=15, title='2023 Iberian Peninsula', start_date=datetime.datetime(2023, 4, 22, 0, 0), end_date=datetime.datetime(2023, 5, 1, 0, 0), location=CenteredRegion(latitude=41.1496, longitude=352.389, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=16, title='2023 Gibraltar', start_date=datetime.datetime(2023, 4, 22, 0, 0), end_date=datetime.datetime(2023, 5, 3, 0, 0), location=CenteredRegion(latitude=36.1408, longitude=354.64639999999997, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=17, title='2023 India', start_date=datetime.datetime(2023, 2, 15, 0, 0), end_date=datetime.datetime(2023, 3, 1, 0, 0), location=CenteredRegion(latitude=28.6139, longitude=77.209, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=18, title='2021 Western Russia', start_date=datetime.datetime(2021, 6, 18, 0, 0), end_date=datetime.datetime(2021, 6, 30, 0, 0), location=CenteredRegion(latitude=55.7558, longitude=37.6173, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=19, title='2022 Germany', start_date=datetime.datetime(2022, 12, 23, 0, 0), end_date=datetime.datetime(2022, 12, 31, 0, 0), location=CenteredRegion(latitude=52.52, longitude=13.405, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=20, title='2022 UK (August)', start_date=datetime.datetime(2022, 8, 8, 0, 0), end_date=datetime.datetime(2022, 8, 16, 0, 0), location=CenteredRegion(latitude=51.5074, longitude=359.8722, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=21, title='2022 UK (July)', start_date=datetime.datetime(2022, 7, 15, 0, 0), end_date=datetime.datetime(2022, 7, 23, 0, 0), location=CenteredRegion(latitude=51.5074, longitude=359.8722, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=22, title='2022 France', start_date=datetime.datetime(2022, 6, 9, 0, 0), end_date=datetime.datetime(2022, 6, 21, 0, 0), location=CenteredRegion(latitude=43.4832, longitude=358.4414, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=23, title='2022 Japan', start_date=datetime.datetime(2022, 6, 20, 0, 0), end_date=datetime.datetime(2022, 7, 5, 0, 0), location=CenteredRegion(latitude=35.6895, longitude=139.6917, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=24, title='2022 India', start_date=datetime.datetime(2022, 4, 24, 0, 0), end_date=datetime.datetime(2022, 5, 4, 0, 0), location=CenteredRegion(latitude=28.2769, longitude=68.4376, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=25, title='2022 East Antarctica', start_date=datetime.datetime(2022, 3, 12, 0, 0), end_date=datetime.datetime(2022, 3, 26, 0, 0), location=CenteredRegion(latitude=-75.1, longitude=123.35, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=26, title='2022 West Australia', start_date=datetime.datetime(2022, 1, 15, 0, 0), end_date=datetime.datetime(2022, 1, 25, 0, 0), location=CenteredRegion(latitude=-31.9505, longitude=115.8605, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=27, title='2021 Canada Plains', start_date=datetime.datetime(2021, 5, 30, 0, 0), end_date=datetime.datetime(2021, 6, 9, 0, 0), location=CenteredRegion(latitude=49.0, longitude=262.43330000000003, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=28, title='2021 New Zealand', start_date=datetime.datetime(2021, 1, 12, 18, 0), end_date=datetime.datetime(2021, 1, 17, 18, 0), location=CenteredRegion(latitude=-43.8983, longitude=171.731, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=29, title='2020 Australia', start_date=datetime.datetime(2020, 11, 25, 0, 0), end_date=datetime.datetime(2020, 11, 30, 0, 0), location=CenteredRegion(latitude=-33.8245, longitude=150.9448, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=72, title='May 2024 Texas', start_date=datetime.datetime(2024, 5, 25, 0, 0), end_date=datetime.datetime(2024, 5, 28, 0, 0), location=CenteredRegion(latitude=25.9017, longitude=262.50260000000003, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=73, title='June 2024 Northeast US', start_date=datetime.datetime(2024, 6, 17, 0, 0), end_date=datetime.datetime(2024, 6, 22, 0, 0), location=CenteredRegion(latitude=41.8781, longitude=286.771, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=74, title='July 2024 Southwest US', start_date=datetime.datetime(2024, 7, 4, 0, 0), end_date=datetime.datetime(2024, 7, 8, 0, 0), location=CenteredRegion(latitude=33.7701, longitude=243.78539999999998, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=75, title='July 2024 Northeast US', start_date=datetime.datetime(2024, 7, 7, 0, 0), end_date=datetime.datetime(2024, 7, 10, 0, 0), location=CenteredRegion(latitude=40.7128, longitude=285.994, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=76, title='July 2024 Mid-Atlantic US', start_date=datetime.datetime(2024, 7, 15, 0, 0), end_date=datetime.datetime(2024, 7, 20, 0, 0), location=CenteredRegion(latitude=39.9526, longitude=284.8348, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=77, title='August 2024 Midwest US', start_date=datetime.datetime(2024, 8, 25, 0, 0), end_date=datetime.datetime(2024, 8, 31, 0, 0), location=CenteredRegion(latitude=40.1106, longitude=271.79269999999997, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=78, title='July 2024 Antarctica', start_date=datetime.datetime(2024, 7, 1, 0, 0), end_date=datetime.datetime(2024, 7, 31, 0, 0), location=CenteredRegion(latitude=-75.0, longitude=15.0, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=79, title='August 2024 Canada', start_date=datetime.datetime(2024, 8, 9, 0, 0), end_date=datetime.datetime(2024, 8, 11, 0, 0), location=CenteredRegion(latitude=67.0, longitude=248.0, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=80, title='August 2024 Australia', start_date=datetime.datetime(2024, 8, 22, 0, 0), end_date=datetime.datetime(2024, 8, 30, 0, 0), location=CenteredRegion(latitude=-20.0, longitude=120.0, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=81, title='July/August 2024 Japan', start_date=datetime.datetime(2024, 7, 25, 0, 0), end_date=datetime.datetime(2024, 8, 5, 0, 0), location=CenteredRegion(latitude=36.0, longitude=138.0, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=82, title='June 2024 Saudi Arabia', start_date=datetime.datetime(2024, 6, 16, 0, 0), end_date=datetime.datetime(2024, 6, 18, 0, 0), location=CenteredRegion(latitude=24.0, longitude=45.0, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=83, title='August 2024 Europe', start_date=datetime.datetime(2024, 8, 10, 0, 0), end_date=datetime.datetime(2024, 8, 15, 0, 0), location=CenteredRegion(latitude=48.3794, longitude=10.8978, bounding_box_degrees=10), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=84, title='July 2024 Ukraine', start_date=datetime.datetime(2024, 7, 12, 0, 0), end_date=datetime.datetime(2024, 7, 16, 0, 0), location=CenteredRegion(latitude=50.4501, longitude=30.5234, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=85, title='June 2024 Europe', start_date=datetime.datetime(2024, 6, 20, 0, 0), end_date=datetime.datetime(2024, 6, 30, 0, 0), location=CenteredRegion(latitude=52.3676, longitude=4.9041, bounding_box_degrees=6), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=86, title='May 2024 Central Mexico', start_date=datetime.datetime(2024, 5, 23, 0, 0), end_date=datetime.datetime(2024, 5, 31, 0, 0), location=CenteredRegion(latitude=19.4326, longitude=260.8668, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=87, title='May 2024 Pakistan/India', start_date=datetime.datetime(2024, 5, 23, 0, 0), end_date=datetime.datetime(2024, 5, 31, 0, 0), location=CenteredRegion(latitude=34.0, longitude=76.0, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=88, title='August 2023 Chile', start_date=datetime.datetime(2023, 8, 1, 0, 0), end_date=datetime.datetime(2023, 8, 3, 0, 0), location=CenteredRegion(latitude=-33.4489, longitude=289.3307, bounding_box_degrees=5), event_type='heat_wave', data_vars=None, cross_listed=None), metrics=[<class '__main__.MaximumMAE'>, <class '__main__.MaxMinMAE'>, <class '__main__.RegionalRMSE'>, <class '__main__.OnsetME'>, <class '__main__.DurationME'>], observations=[<class '__main__.ERA5'>]), CaseOperator(case=IndividualCase(case_id_number=36, title='July 2024 South Dakota', start_date=datetime.datetime(2024, 7, 13, 0, 0), end_date=datetime.datetime(2024, 7, 14, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=49.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=37, title='July 2024 Chicago', start_date=datetime.datetime(2024, 7, 14, 0, 0), end_date=datetime.datetime(2024, 7, 16, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=49.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=38, title='July 2024 New York', start_date=datetime.datetime(2024, 7, 16, 0, 0), end_date=datetime.datetime(2024, 7, 17, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=49.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=39, title='May 2024 Wisconsin', start_date=datetime.datetime(2024, 5, 18, 0, 0), end_date=datetime.datetime(2024, 5, 19, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=40, title='May 2024 Kansas', start_date=datetime.datetime(2024, 5, 19, 0, 0), end_date=datetime.datetime(2024, 5, 20, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=41, title='May 2024 Iowa + Nebraska', start_date=datetime.datetime(2024, 5, 20, 0, 0), end_date=datetime.datetime(2024, 5, 21, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=42, title='May 2024 Iowa + Wisconsin', start_date=datetime.datetime(2024, 5, 21, 0, 0), end_date=datetime.datetime(2024, 5, 22, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=43, title='May 2024 Texas', start_date=datetime.datetime(2024, 5, 22, 0, 0), end_date=datetime.datetime(2024, 5, 23, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=44, title='May 2024 Iowa + Nebraska 2', start_date=datetime.datetime(2024, 5, 23, 0, 0), end_date=datetime.datetime(2024, 5, 24, 0, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=45, title='April 2024 Oklahoma', start_date=datetime.datetime(2024, 4, 25, 12, 0), end_date=datetime.datetime(2024, 4, 29, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=46, title='April 2024 Midwest', start_date=datetime.datetime(2024, 4, 2, 12, 0), end_date=datetime.datetime(2024, 4, 3, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=47, title='April 2024 Great Plains + Midwest', start_date=datetime.datetime(2024, 4, 1, 12, 0), end_date=datetime.datetime(2024, 4, 2, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=48, title='March 2024 Midwest', start_date=datetime.datetime(2024, 3, 14, 12, 0), end_date=datetime.datetime(2024, 3, 15, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=49, title='February 2024 Midwest', start_date=datetime.datetime(2024, 2, 27, 12, 0), end_date=datetime.datetime(2024, 2, 28, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=50, title='January 2024 Southeast', start_date=datetime.datetime(2024, 1, 8, 12, 0), end_date=datetime.datetime(2024, 1, 10, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=51, title='December 2023 South', start_date=datetime.datetime(2023, 12, 9, 12, 0), end_date=datetime.datetime(2023, 12, 10, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=52, title='August 2023 East Coast', start_date=datetime.datetime(2023, 8, 7, 12, 0), end_date=datetime.datetime(2023, 8, 8, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=53, title='August 2023 Minnesota', start_date=datetime.datetime(2023, 8, 11, 12, 0), end_date=datetime.datetime(2023, 8, 12, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=54, title='July 2023 Midwest', start_date=datetime.datetime(2023, 7, 2, 12, 0), end_date=datetime.datetime(2023, 7, 3, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=55, title='July 2023 Midwest 2', start_date=datetime.datetime(2023, 7, 1, 12, 0), end_date=datetime.datetime(2023, 7, 2, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=56, title='June 2023 Midwest', start_date=datetime.datetime(2023, 6, 30, 12, 0), end_date=datetime.datetime(2023, 7, 1, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=57, title='June 2023 Front Range', start_date=datetime.datetime(2023, 6, 21, 12, 0), end_date=datetime.datetime(2023, 6, 22, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=58, title='June 2023 Texas', start_date=datetime.datetime(2023, 6, 21, 12, 0), end_date=datetime.datetime(2023, 6, 22, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=59, title='June 2023 Great Plains', start_date=datetime.datetime(2023, 6, 23, 12, 0), end_date=datetime.datetime(2023, 6, 24, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=60, title='June 2023 Iowa + Minnesota', start_date=datetime.datetime(2023, 6, 24, 12, 0), end_date=datetime.datetime(2023, 6, 25, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=61, title='June 2023 South + Midwest', start_date=datetime.datetime(2023, 6, 25, 12, 0), end_date=datetime.datetime(2023, 6, 26, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=62, title='June 2023 East Coast', start_date=datetime.datetime(2023, 6, 26, 12, 0), end_date=datetime.datetime(2023, 6, 27, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=63, title='June 2023 Midwest + Great Plains', start_date=datetime.datetime(2023, 6, 29, 12, 0), end_date=datetime.datetime(2023, 6, 30, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=64, title='June 2023 Southeast', start_date=datetime.datetime(2023, 6, 14, 12, 0), end_date=datetime.datetime(2023, 6, 15, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=65, title='June 2023 OK + TX + South', start_date=datetime.datetime(2023, 6, 15, 12, 0), end_date=datetime.datetime(2023, 6, 16, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=66, title='June 2023 South', start_date=datetime.datetime(2023, 6, 16, 12, 0), end_date=datetime.datetime(2023, 6, 17, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=67, title='June 2023 Mid-Atlantic', start_date=datetime.datetime(2023, 6, 16, 12, 0), end_date=datetime.datetime(2023, 6, 17, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=68, title='June 2023 Great Plains + South', start_date=datetime.datetime(2023, 6, 17, 12, 0), end_date=datetime.datetime(2023, 6, 18, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=69, title='May 2023 Great Plains', start_date=datetime.datetime(2023, 5, 10, 12, 0), end_date=datetime.datetime(2023, 5, 11, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=70, title='May 2023 Great Plains + South', start_date=datetime.datetime(2023, 5, 11, 12, 0), end_date=datetime.datetime(2023, 5, 12, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>]), CaseOperator(case=IndividualCase(case_id_number=71, title='May 2023 Nebraska', start_date=datetime.datetime(2023, 5, 12, 12, 0), end_date=datetime.datetime(2023, 5, 13, 12, 0), location=BoundingBoxRegion(latitude_min=24.0, latitude_max=54.0, longitude_min=245.0, longitude_max=295.0), event_type='severe_convection', data_vars=None, cross_listed=None), metrics=[<class '__main__.CSI'>, <class '__main__.LeadTimeDetection'>, <class '__main__.RegionalHitsMisses'>, <class '__main__.HitsMisses'>], observations=[<class '__main__.LSR'>])])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewb.event_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<xarray.Dataset> Size: 503kB\n",
      "Dimensions:                  (time: 313, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 50.0 49.75 ... 45.5 45.25\n",
      "  * longitude                (longitude) float32 80B 235.2 235.5 ... 239.8 240.0\n",
      "  * time                     (time) datetime64[ns] 3kB 2021-06-20 ... 2021-07-03\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 501kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 388kB\n",
      "Dimensions:                  (time: 241, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 44.25 44.0 ... 39.75 39.5\n",
      "  * longitude                (longitude) float32 80B 270.0 270.2 ... 274.5 274.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-05-07 ... 2022-05-17\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 386kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 36.5 36.25 ... 32.0 31.75\n",
      "  * longitude                (longitude) float32 80B 239.5 239.8 ... 244.0 244.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-06-07 ... 2022-06-15\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 696kB\n",
      "Dimensions:                  (time: 433, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 35.25 35.0 ... 30.75 30.5\n",
      "  * longitude                (longitude) float32 80B 260.8 261.0 ... 265.2 265.5\n",
      "  * time                     (time) datetime64[ns] 3kB 2022-06-30 ... 2022-07-18\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 693kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 503kB\n",
      "Dimensions:                  (time: 313, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 50.0 49.75 ... 45.5 45.25\n",
      "  * longitude                (longitude) float32 80B 235.2 235.5 ... 239.8 240.0\n",
      "  * time                     (time) datetime64[ns] 3kB 2023-05-10 ... 2023-05-23\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 501kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 272kB\n",
      "Dimensions:                  (time: 169, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 41.75 41.5 ... 37.25 37.0\n",
      "  * longitude                (longitude) float32 80B 281.0 281.2 ... 285.5 285.8\n",
      "  * time                     (time) datetime64[ns] 1kB 2022-05-17 ... 2022-05-24\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 270kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 388kB\n",
      "Dimensions:                  (time: 241, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -29.5 -29.75 ... -34.25\n",
      "  * longitude                (longitude) float32 80B 113.5 113.8 ... 118.0 118.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-11-18 ... 2023-11-28\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 386kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 426kB\n",
      "Dimensions:                  (time: 265, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 55.5 55.25 ... 51.0 50.75\n",
      "  * longitude                (longitude) float32 80B 350.0 350.2 ... 354.5 354.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-09-02 ... 2023-09-13\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 424kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 774kB\n",
      "Dimensions:                  (time: 481, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 44.25 44.0 ... 39.75 39.5\n",
      "  * longitude                (longitude) float32 80B 10.0 10.25 ... 14.5 14.75\n",
      "  * time                     (time) datetime64[ns] 4kB 2023-07-07 ... 2023-07-27\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 770kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 426kB\n",
      "Dimensions:                  (time: 265, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 42.75 42.5 ... 38.25 38.0\n",
      "  * longitude                (longitude) float32 80B 354.0 354.2 ... 358.5 358.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-08-17 ... 2023-08-28\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 424kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 233kB\n",
      "Dimensions:                  (time: 145, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -32.25 -32.5 ... -37.0\n",
      "  * longitude                (longitude) float32 80B 299.2 299.5 ... 303.8 304.0\n",
      "  * time                     (time) datetime64[ns] 1kB 2023-07-29 ... 2023-08-04\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 232kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 33.5 33.25 ... 29.0 28.75\n",
      "  * longitude                (longitude) float32 80B 119.0 119.2 ... 123.5 123.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-05-24 ... 2023-06-01\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 349kB\n",
      "Dimensions:                  (time: 217, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 27.5 27.25 ... 23.0 22.75\n",
      "  * longitude                (longitude) float32 80B 100.2 100.5 ... 104.8 105.0\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-04-14 ... 2023-04-23\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 347kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 851kB\n",
      "Dimensions:                  (time: 529, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 39.0 38.75 ... 34.5 34.25\n",
      "  * longitude                (longitude) float32 80B 0.75 1.0 1.25 ... 5.25 5.5\n",
      "  * time                     (time) datetime64[ns] 4kB 2023-07-05 ... 2023-07-27\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 846kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 349kB\n",
      "Dimensions:                  (time: 217, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 43.5 43.25 ... 39.0 38.75\n",
      "  * longitude                (longitude) float32 80B 350.0 350.2 ... 354.5 354.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-04-22 ... 2023-05-01\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 347kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 426kB\n",
      "Dimensions:                  (time: 265, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 38.5 38.25 ... 34.0 33.75\n",
      "  * longitude                (longitude) float32 80B 352.2 352.5 ... 356.8 357.0\n",
      "  * time                     (time) datetime64[ns] 2kB 2023-04-22 ... 2023-05-03\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 424kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 542kB\n",
      "Dimensions:                  (time: 337, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 31.0 30.75 ... 26.5 26.25\n",
      "  * longitude                (longitude) float32 80B 74.75 75.0 ... 79.25 79.5\n",
      "  * time                     (time) datetime64[ns] 3kB 2023-02-15 ... 2023-03-01\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 539kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 465kB\n",
      "Dimensions:                  (time: 289, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 58.25 58.0 ... 53.75 53.5\n",
      "  * longitude                (longitude) float32 80B 35.25 35.5 ... 39.75 40.0\n",
      "  * time                     (time) datetime64[ns] 2kB 2021-06-18 ... 2021-06-30\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 462kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 55.0 54.75 ... 50.5 50.25\n",
      "  * longitude                (longitude) float32 80B 11.0 11.25 ... 15.5 15.75\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-12-23 ... 2022-12-31\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 22MB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 1420)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 54.0 53.75 ... 49.5 49.25\n",
      "  * longitude                (longitude) float32 6kB 2.5 2.75 ... 357.0 357.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-08-08 ... 2022-08-16\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 22MB dask.array<chunksize=(48, 20, 1420), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 22MB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 1420)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 54.0 53.75 ... 49.5 49.25\n",
      "  * longitude                (longitude) float32 6kB 2.5 2.75 ... 357.0 357.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-07-15 ... 2022-07-23\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 22MB dask.array<chunksize=(48, 20, 1420), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 33MB\n",
      "Dimensions:                  (time: 289, latitude: 20, longitude: 1420)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 45.75 45.5 ... 41.25 41.0\n",
      "  * longitude                (longitude) float32 6kB 1.0 1.25 ... 355.5 355.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-06-09 ... 2022-06-21\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 33MB dask.array<chunksize=(48, 20, 1420), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 581kB\n",
      "Dimensions:                  (time: 361, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 38.0 37.75 ... 33.5 33.25\n",
      "  * longitude                (longitude) float32 80B 137.2 137.5 ... 141.8 142.0\n",
      "  * time                     (time) datetime64[ns] 3kB 2022-06-20 ... 2022-07-05\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 578kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 388kB\n",
      "Dimensions:                  (time: 241, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 30.75 30.5 ... 26.25 26.0\n",
      "  * longitude                (longitude) float32 80B 66.0 66.25 ... 70.5 70.75\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-04-24 ... 2022-05-04\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 386kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 542kB\n",
      "Dimensions:                  (time: 337, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -72.75 -73.0 ... -77.5\n",
      "  * longitude                (longitude) float32 80B 121.0 121.2 ... 125.5 125.8\n",
      "  * time                     (time) datetime64[ns] 3kB 2022-03-12 ... 2022-03-26\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 539kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 388kB\n",
      "Dimensions:                  (time: 241, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -29.5 -29.75 ... -34.25\n",
      "  * longitude                (longitude) float32 80B 113.5 113.8 ... 118.0 118.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2022-01-15 ... 2022-01-25\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 386kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 388kB\n",
      "Dimensions:                  (time: 241, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 51.5 51.25 ... 47.0 46.75\n",
      "  * longitude                (longitude) float32 80B 260.0 260.2 ... 264.5 264.8\n",
      "  * time                     (time) datetime64[ns] 2kB 2021-05-30 ... 2021-06-09\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 386kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 195kB\n",
      "Dimensions:                  (time: 121, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -41.5 -41.75 ... -46.25\n",
      "  * longitude                (longitude) float32 80B 169.2 169.5 ... 173.8 174.0\n",
      "  * time                     (time) datetime64[ns] 968B 2021-01-12T18:00:00 ....\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 194kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 195kB\n",
      "Dimensions:                  (time: 121, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -31.5 -31.75 ... -36.25\n",
      "  * longitude                (longitude) float32 80B 148.5 148.8 ... 153.0 153.2\n",
      "  * time                     (time) datetime64[ns] 968B 2020-11-25 ... 2020-1...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 194kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 118kB\n",
      "Dimensions:                  (time: 73, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 28.25 28.0 ... 23.75 23.5\n",
      "  * longitude                (longitude) float32 80B 260.2 260.5 ... 264.8 265.0\n",
      "  * time                     (time) datetime64[ns] 584B 2024-05-25 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 117kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 280kB\n",
      "Dimensions:                  (time: 121, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 44.75 44.5 ... 39.25 39.0\n",
      "  * longitude                (longitude) float32 96B 284.0 284.2 ... 289.5 289.8\n",
      "  * time                     (time) datetime64[ns] 968B 2024-06-17 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 279kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 224kB\n",
      "Dimensions:                  (time: 97, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 36.75 36.5 ... 31.25 31.0\n",
      "  * longitude                (longitude) float32 96B 241.0 241.2 ... 246.5 246.8\n",
      "  * time                     (time) datetime64[ns] 776B 2024-07-04 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 223kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 169kB\n",
      "Dimensions:                  (time: 73, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 43.5 43.25 ... 38.0 37.75\n",
      "  * longitude                (longitude) float32 96B 283.0 283.2 ... 288.5 288.8\n",
      "  * time                     (time) datetime64[ns] 584B 2024-07-07 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 168kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 280kB\n",
      "Dimensions:                  (time: 121, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 42.75 42.5 ... 37.25 37.0\n",
      "  * longitude                (longitude) float32 96B 282.0 282.2 ... 287.5 287.8\n",
      "  * time                     (time) datetime64[ns] 968B 2024-07-15 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 279kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 233kB\n",
      "Dimensions:                  (time: 145, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 42.5 42.25 ... 38.0 37.75\n",
      "  * longitude                (longitude) float32 80B 269.5 269.8 ... 274.0 274.2\n",
      "  * time                     (time) datetime64[ns] 1kB 2024-08-25 ... 2024-08-31\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 232kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 1MB\n",
      "Dimensions:                  (time: 721, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -72.5 -72.75 ... -77.25\n",
      "  * longitude                (longitude) float32 80B 12.75 13.0 ... 17.25 17.5\n",
      "  * time                     (time) datetime64[ns] 6kB 2024-07-01 ... 2024-07-31\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 1MB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 79kB\n",
      "Dimensions:                  (time: 49, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 69.5 69.25 ... 65.0 64.75\n",
      "  * longitude                (longitude) float32 80B 245.8 246.0 ... 250.2 250.5\n",
      "  * time                     (time) datetime64[ns] 392B 2024-08-09 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 78kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -17.5 -17.75 ... -22.25\n",
      "  * longitude                (longitude) float32 80B 117.8 118.0 ... 122.2 122.5\n",
      "  * time                     (time) datetime64[ns] 2kB 2024-08-22 ... 2024-08-30\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 613kB\n",
      "Dimensions:                  (time: 265, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 39.0 38.75 ... 33.5 33.25\n",
      "  * longitude                (longitude) float32 96B 135.2 135.5 ... 140.8 141.0\n",
      "  * time                     (time) datetime64[ns] 2kB 2024-07-25 ... 2024-08-05\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 611kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 113kB\n",
      "Dimensions:                  (time: 49, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 27.0 26.75 ... 21.5 21.25\n",
      "  * longitude                (longitude) float32 96B 42.25 42.5 ... 47.75 48.0\n",
      "  * time                     (time) datetime64[ns] 392B 2024-06-16 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 113kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 776kB\n",
      "Dimensions:                  (time: 121, latitude: 40, longitude: 40)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 160B 53.25 53.0 ... 43.75 43.5\n",
      "  * longitude                (longitude) float32 160B 6.0 6.25 ... 15.5 15.75\n",
      "  * time                     (time) datetime64[ns] 968B 2024-08-10 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 774kB dask.array<chunksize=(48, 40, 40), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 156kB\n",
      "Dimensions:                  (time: 97, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 52.75 52.5 ... 48.25 48.0\n",
      "  * longitude                (longitude) float32 80B 28.25 28.5 ... 32.75 33.0\n",
      "  * time                     (time) datetime64[ns] 776B 2024-07-12 ... 2024-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 155kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 557kB\n",
      "Dimensions:                  (time: 241, latitude: 24, longitude: 24)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 96B 55.25 55.0 ... 49.75 49.5\n",
      "  * longitude                (longitude) float32 96B 2.0 2.25 2.5 ... 7.5 7.75\n",
      "  * time                     (time) datetime64[ns] 2kB 2024-06-20 ... 2024-06-30\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 555kB dask.array<chunksize=(48, 24, 24), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 21.75 21.5 ... 17.25 17.0\n",
      "  * longitude                (longitude) float32 80B 258.5 258.8 ... 263.0 263.2\n",
      "  * time                     (time) datetime64[ns] 2kB 2024-05-23 ... 2024-05-31\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 311kB\n",
      "Dimensions:                  (time: 193, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B 36.5 36.25 ... 32.0 31.75\n",
      "  * longitude                (longitude) float32 80B 73.75 74.0 ... 78.25 78.5\n",
      "  * time                     (time) datetime64[ns] 2kB 2024-05-23 ... 2024-05-31\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 309kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 79kB\n",
      "Dimensions:                  (time: 49, latitude: 20, longitude: 20)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 80B -31.0 -31.25 ... -35.75\n",
      "  * longitude                (longitude) float32 80B 287.0 287.2 ... 291.5 291.8\n",
      "  * time                     (time) datetime64[ns] 392B 2023-08-01 ... 2023-0...\n",
      "Data variables:\n",
      "    surface_air_temperature  (time, latitude, longitude) float32 78kB dask.array<chunksize=(48, 20, 20), meta=np.ndarray>\n",
      "Attributes:\n",
      "    last_updated:           2025-07-25 01:52:37.403950+00:00\n",
      "    valid_time_start:       1940-01-01\n",
      "    valid_time_stop:        2025-04-30\n",
      "    valid_time_stop_era5t:  2025-07-19]\n",
      "[<xarray.Dataset> Size: 327kB\n",
      "Dimensions:              (latitude: 101, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 808B 24.0 24.25 24.5 ... 48.75 49.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 327kB\n",
      "Dimensions:              (latitude: 101, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 808B 24.0 24.25 24.5 ... 48.75 49.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 327kB\n",
      "Dimensions:              (latitude: 101, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 808B 24.0 24.25 24.5 ... 48.75 49.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 162kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n",
      "[<xarray.Dataset> Size: 392kB\n",
      "Dimensions:              (latitude: 121, longitude: 201)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float64 968B 24.0 24.25 24.5 ... 53.75 54.0\n",
      "  * longitude            (longitude) float64 2kB 245.0 245.2 ... 294.8 295.0\n",
      "Data variables:\n",
      "    reports              (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0\n",
      "    practically_perfect  (latitude, longitude) float64 195kB 0.0 0.0 ... 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "# args here will apply to all observations and forecasts for all events;\n",
    "# these should also be able to be passed to each event type individually\n",
    "ewb.run(storage_options=dict(token=\"anon\"), chunks={'time': 48, 'latitude': 721, 'longitude': 1440}, variable_mapping=ERA5_MAPPING)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe = SevereConvection(case_metadata=case_yaml)\n",
    "\n",
    "# With the EventOperator class\n",
    "simple_event_operator = EventOperator(events=[severe])\n",
    "ewb = ExtremeWeatherBench(simple_event_operator, forecast_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x100bd85e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/taylor/.local/share/uv/python/cpython-3.13.1-macos-aarch64-none/lib/python3.13/_weakrefset.py\", line 39, in _remove\n",
      "    def _remove(item, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for n in ewb.event_operator.pre_composed_case_operators:\n",
    "    test_output = ewb.process_observations(n)\n",
    "    ax = plot_practically_perfect_hindcast(test_output)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
